{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56d5d551-0fa8-4dcd-81a3-d069f5412e65",
   "metadata": {},
   "source": [
    "## This model findes n embeddings to make gpt2 produce any sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7059ae35-f7c9-4783-8d15-d32429efa309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch\n",
    "from ipywidgets import Dropdown\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "165b60c1-fd2c-4bd7-99bb-1cc4b38e2aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_magic_tokens = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ea33327-92d2-45f1-a518-ec0e8c5398f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d2277d-5758-478d-bdf1-3b0ea2b00046",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load_tokenizer( \"gpt2\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7dead88-6335-462b-9f78-932ee8924b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model( \"gpt2\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "656f0198-b32b-4045-877d-a16962f671e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_index = torch.LongTensor([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2691743-fae2-44ce-9cf8-3126ce181791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The learned magic tokens will end up in this list\n",
    "learned_magic_token_list = []\n",
    "sentence_to_produce = \"This sentence will become an embedding.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef245315-cb7b-46b4-8757-27058d69e5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Working on sentence This sentence will become an embedding.\n",
      "\n",
      "0:  the\n",
      "breaking because  the != This\n",
      "\n",
      "1: The\n",
      "breaking because The != This\n",
      "\n",
      "2: \n",
      "\n",
      "breaking because \n",
      " != This\n",
      "\n",
      "3: The\n",
      "breaking because The != This\n",
      "\n",
      "4: The\n",
      "breaking because The != This\n",
      "\n",
      "5: The\n",
      "breaking because The != This\n",
      "\n",
      "6: The\n",
      "breaking because The != This\n",
      "\n",
      "7: This is\n",
      "breaking because  is !=  sentence\n",
      "\n",
      "8: This.\n",
      "breaking because . !=  sentence\n",
      "\n",
      "9: This is\n",
      "breaking because  is !=  sentence\n",
      "\n",
      "10: This is\n",
      "breaking because  is !=  sentence\n",
      "\n",
      "11: The\n",
      "breaking because The != This\n",
      "\n",
      "12: This.\n",
      "breaking because . !=  sentence\n",
      "\n",
      "13: This is\n",
      "breaking because  is !=  sentence\n",
      "\n",
      "14: This Act\n",
      "breaking because  Act !=  sentence\n",
      "\n",
      "15: This sentence:\n",
      "breaking because : !=  will\n",
      "\n",
      "16: This is\n",
      "breaking because  is !=  sentence\n",
      "\n",
      "17: This sentence is\n",
      "breaking because  is !=  will\n",
      "\n",
      "18: This sentence is\n",
      "breaking because  is !=  will\n",
      "\n",
      "19: This sentence is\n",
      "breaking because  is !=  will\n",
      "\n",
      "20: This sentence will be\n",
      "breaking because  be !=  become\n",
      "\n",
      "21: This is\n",
      "breaking because  is !=  sentence\n",
      "\n",
      "22: This sentence is\n",
      "breaking because  is !=  will\n",
      "\n",
      "23: This sentence will be\n",
      "breaking because  be !=  become\n",
      "\n",
      "24: This sentence will be\n",
      "breaking because  be !=  become\n",
      "\n",
      "25: This sentence will be\n",
      "breaking because  be !=  become\n",
      "\n",
      "26: This sentence will be\n",
      "breaking because  be !=  become\n",
      "\n",
      "27: This sentence will become effective\n",
      "breaking because  effective !=  an\n",
      "\n",
      "28: This sentence is\n",
      "breaking because  is !=  will\n",
      "\n",
      "29: This sentence will be\n",
      "breaking because  be !=  become\n",
      "\n",
      "30: This sentence will be\n",
      "breaking because  be !=  become\n",
      "\n",
      "31: This sentence will become a\n",
      "breaking because  a !=  an\n",
      "\n",
      "32: This sentence will become a\n",
      "breaking because  a !=  an\n",
      "\n",
      "33: This sentence will become an age\n",
      "breaking because  age !=  embed\n",
      "\n",
      "34: This sentence will be\n",
      "breaking because  be !=  become\n",
      "\n",
      "35: This sentence will become a\n",
      "breaking because  a !=  an\n",
      "\n",
      "36: This sentence will become an end\n",
      "breaking because  end !=  embed\n",
      "\n",
      "37: This sentence will become an official\n",
      "breaking because  official !=  embed\n",
      "\n",
      "38: This sentence will be\n",
      "breaking because  be !=  become\n",
      "\n",
      "39: This sentence will become an embed.\n",
      "breaking because . != ding\n",
      "\n",
      "40: This sentence will become a\n",
      "breaking because  a !=  an\n",
      "\n",
      "41: This sentence will become an embeds\n",
      "breaking because s != ding\n",
      "\n",
      "42: This sentence will become an embedding code\n",
      "breaking because  code != .\n",
      "\n",
      "43: This sentence will become an Official\n",
      "breaking because  Official !=  embed\n",
      "\n",
      "44: This sentence will become an embedding."
     ]
    }
   ],
   "source": [
    "print( f\"\\n\\nWorking on sentence {sentence_to_produce}\" )\n",
    "\n",
    "tokenized = tokenizer( sentence_to_produce )\n",
    "\n",
    "slope = -.1\n",
    "\n",
    "\n",
    "for run in range( 100 ):\n",
    "    model.zero_grad()\n",
    "    print( f\"\\n{run}: \", end='' )\n",
    "\n",
    "    \n",
    "    # if learned_magic_token_list:\n",
    "    #     print( f\" First couple learned values of magic token 0: {learned_magic_token_list[0][:5].detach().numpy()}\", end='\\n' )\n",
    "\n",
    "    found_problem = False\n",
    "    \n",
    "    for token_to_teach in range( len(tokenized[\"input_ids\"]) ):\n",
    "\n",
    "        input_ids = torch.LongTensor(tokenized[\"input_ids\"][:token_to_teach])\n",
    "        correct_token = tokenized[\"input_ids\"][token_to_teach]\n",
    "        target_tensor = torch.zeros(len(tokenizer))\n",
    "        target_tensor[correct_token] = 25.0\n",
    "\n",
    "        inputs_embeds_list = []\n",
    "        for magic_token_i in range( num_magic_tokens ):\n",
    "            while magic_token_i >= len( learned_magic_token_list ):\n",
    "                new_magic_token = torch.randn_like( model.get_input_embeddings()(zero_index)[0] )\n",
    "                new_magic_token.requires_grad = True\n",
    "                new_magic_token.retain_grad()\n",
    "                learned_magic_token_list.append( new_magic_token )\n",
    "            inputs_embeds_list.append( learned_magic_token_list[magic_token_i] )\n",
    "        for input_index in range(token_to_teach):\n",
    "            inputs_embeds_list.append( model.get_input_embeddings()( torch.LongTensor([tokenized[\"input_ids\"][input_index]]) )[0] )\n",
    "            \n",
    "        inputs_embeds = torch.stack(inputs_embeds_list, dim=0)\n",
    "        #inputs_embeds.requires_grad = True\n",
    "        #inputs_embeds.retain_grad()\n",
    "        result = model.forward( inputs_embeds = inputs_embeds )\n",
    "        loss = F.cross_entropy( result.logits[-1].unsqueeze(0), target_tensor.unsqueeze(0) )\n",
    "        loss.backward()\n",
    "    \n",
    "        probs = F.softmax(result.logits[-1], dim=-1)\n",
    "        #sampled_token_id = torch.multinomial(probs, 1).item()\n",
    "        sampled_token_id = torch.argmax(probs, dim=-1).item()\n",
    "        print(tokenizer.decode( [sampled_token_id] ), end='')\n",
    "\n",
    "        if sampled_token_id != correct_token:\n",
    "            found_problem = True\n",
    "            print( f\"\\nbreaking because {tokenizer.decode([sampled_token_id])} != {tokenizer.decode([correct_token])}\" )\n",
    "            break\n",
    "    \n",
    "    # new_inputs_embeds = inputs_embeds + (inputs_embeds.grad * slope)\n",
    "\n",
    "    # for magic_token_i in range(len(magic_tokens)):\n",
    "    #     magic_token = magic_tokens[magic_token_i]\n",
    "    #     learned_magic_tokens[magic_token] = new_inputs_embeds[magic_token_i].detach()\n",
    "\n",
    "    for magic_token_i in range( num_magic_tokens ):\n",
    "        magic_token_tensor = learned_magic_token_list[magic_token_i]\n",
    "        new_magic_token_tensor = (magic_token_tensor + (magic_token_tensor.grad * slope)).detach()\n",
    "        new_magic_token_tensor.requires_grad = True\n",
    "        new_magic_token_tensor.retain_grad()\n",
    "        learned_magic_token_list[magic_token_i] = new_magic_token_tensor\n",
    "\n",
    "    if not found_problem: break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e13c6ef-c42e-49f5-8745-b9020c63b517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding for the sentence This sentence will become an embedding. is \n",
      "[tensor([ 0.1929, -0.5231, -0.0760,  2.6223,  1.9157,  2.9033,  0.1211, -0.1061,\n",
      "         0.6001, -1.7923,  1.7988, -1.3788, -1.7105,  0.1941,  1.6227,  1.2291,\n",
      "         0.5562,  1.9356, -0.7665,  1.6864,  0.1316,  1.5271,  3.0311, -0.0748,\n",
      "        -2.1802, -1.1159,  0.7396, -0.2184,  1.1878,  1.6763, -1.9479, -0.7508,\n",
      "         1.4421,  0.8478,  0.4187,  1.3319, -1.2282,  2.0423, -1.9388, -1.5317,\n",
      "        -1.0179,  1.6385,  0.8261, -1.7613,  0.9579, -0.3701, -1.8082,  1.3161,\n",
      "         0.1808,  1.5822, -2.0018,  1.0880,  0.6485,  1.5574,  0.3187, -1.6992,\n",
      "         1.7380, -0.5354,  1.0756,  0.5897, -1.1188,  1.7005,  0.7067, -0.6838,\n",
      "         0.8132, -0.2790,  2.1543,  0.5475,  0.0438,  3.2126,  1.0016,  2.1984,\n",
      "         1.1711, -1.3954,  0.2343,  0.9718,  0.1050,  0.0598,  0.4093,  2.1276,\n",
      "        -0.0687, -0.1722, -1.8237,  1.9616, -0.8970, -0.1906,  0.0222,  1.4738,\n",
      "         2.8357, -0.6409, -0.9830,  2.0602, -0.0528,  0.0253, -0.3317,  1.2282,\n",
      "        -0.7881,  0.2917,  0.9117, -0.7831, -0.8373,  0.5808,  0.8213,  1.6018,\n",
      "        -0.3293,  0.4894,  1.5460, -0.2938, -0.8537,  0.0819, -0.8881, -1.1126,\n",
      "         3.0316,  1.2384, -1.7933, -2.8893, -0.9828, -1.6532, -0.4960, -1.3840,\n",
      "        -0.1286, -0.2411, -0.7056, -1.6204, -1.4727, -0.9373,  0.2561,  0.9312,\n",
      "        -1.3853,  1.7330, -1.4223, -0.3324,  0.2860,  0.0270, -0.3305,  0.7302,\n",
      "        -0.4269,  0.3815, -1.9316,  0.1998,  0.4797,  0.4671, -0.1185,  1.3963,\n",
      "        -0.5919,  1.1626, -0.2373,  1.3362,  0.8235,  0.9521,  0.4407, -1.9561,\n",
      "         0.3840,  1.3579, -1.2021,  0.6330, -0.7925, -3.1278, -1.6555,  0.9831,\n",
      "         2.7064,  0.8724,  0.2497, -0.8867, -0.5167,  1.7759,  2.6319,  0.2533,\n",
      "         2.3925, -0.9447, -0.5339,  0.8663, -1.1248, -1.6276,  0.1675, -0.6862,\n",
      "         0.3171, -0.5272,  0.2360, -0.4470, -1.3943,  0.7001, -1.3936, -0.7580,\n",
      "        -1.8146,  0.2736, -1.1830,  1.0810,  0.8478,  0.7327,  0.9912,  0.4946,\n",
      "        -1.5640, -0.3593,  2.9194, -1.4729,  0.2477, -0.4599, -0.2198, -0.5500,\n",
      "         0.9727, -0.7836,  1.3971,  0.6067,  1.0171,  0.2678,  1.6394,  1.3690,\n",
      "         0.1798,  1.7010,  0.6117,  0.6218, -1.4273, -0.7999, -0.5250,  0.3277,\n",
      "        -0.5581, -3.0353,  0.1484, -0.1256, -0.4544, -0.5350,  0.2598, -0.1849,\n",
      "        -1.3926,  2.0228,  0.6808,  0.3508, -0.1546, -1.6925, -1.7913,  1.5637,\n",
      "         0.8341,  0.9192, -0.4635,  1.4632, -0.1292,  0.1571,  1.5376, -0.0855,\n",
      "         0.0917, -0.5542, -1.6884,  0.2410, -2.3638, -0.5042, -1.6818, -1.5379,\n",
      "         0.9880,  1.0166,  0.0536,  0.9210,  0.3557,  0.3014, -2.1008, -0.1252,\n",
      "        -1.1540, -2.2763,  0.5250,  0.3226, -1.2995,  0.7130, -1.8476, -1.7843,\n",
      "        -0.5063,  0.2222,  0.2452,  0.3601,  0.7360,  0.2319,  0.8146,  1.5243,\n",
      "        -0.9183, -1.3040, -1.1254,  2.2080, -1.7606,  0.1281,  0.7580,  0.0636,\n",
      "         1.0169,  0.5701,  1.2335,  0.1297,  0.4455,  0.7506,  1.8774, -1.1951,\n",
      "        -1.7786,  0.5493, -0.4437, -2.0202,  1.7689,  1.7565,  0.8987, -1.7078,\n",
      "         1.8972, -0.1962,  1.3307,  2.3650,  1.0036, -0.2597, -0.5013,  0.3577,\n",
      "        -0.3538,  0.0846,  2.6411, -0.9960, -0.9271,  1.3254,  2.3771, -0.5093,\n",
      "         0.0900,  0.4016,  2.9330,  1.5971, -1.3132, -1.0865, -1.1620,  0.5475,\n",
      "         0.6479, -0.3964, -0.7920, -2.0906, -0.6248, -0.3885,  0.0576, -1.3159,\n",
      "         1.2183, -0.3017,  2.2632,  0.5272,  1.7002, -1.2700,  1.0726, -0.8704,\n",
      "        -0.8884,  1.5868,  2.0094,  0.2593, -0.6002, -0.0390, -0.2021,  0.7423,\n",
      "         0.2603, -1.1605,  3.1872,  0.2799,  1.8544, -0.7746,  3.5186, -0.4810,\n",
      "        -1.1719, -2.7036,  3.0377,  0.3198, -1.1880,  0.4544,  1.7623,  0.0404,\n",
      "         1.1825, -1.6766,  0.7844, -2.4160,  0.2146, -1.2905, -0.8733,  0.0271,\n",
      "        -0.6814, -1.1796,  0.1142,  0.1677, -1.0181, -0.3610, -0.2998, -1.5449,\n",
      "         2.2136, -1.5684,  0.5818,  1.5293, -1.9014,  0.9874, -0.0818,  1.4354,\n",
      "        -0.5284,  0.5111,  1.1011,  0.1692,  0.2101,  0.6262,  1.7908,  0.0275,\n",
      "        -0.9000,  0.1461,  0.8013,  0.0999, -1.9300,  0.8776,  0.7038,  0.7785,\n",
      "        -1.8060, -0.0161, -0.2059,  0.3109, -0.6214,  0.7468, -0.3322,  0.4288,\n",
      "         1.4446,  3.2305,  0.2395, -0.3680, -1.8938,  1.6283, -0.3575, -0.5537,\n",
      "        -2.0305,  0.1265,  0.4408, -0.5036,  0.0874, -2.2464, -0.2907,  0.0337,\n",
      "        -1.4469,  1.0884, -0.6743, -0.1976, -1.3234, -0.5952,  0.3768,  1.8199,\n",
      "         0.4264, -0.1189, -0.8704, -1.7809,  0.0697, -0.4857,  0.5064,  1.1964,\n",
      "        -0.4620,  0.8529,  0.1686, -2.8910,  0.8422,  0.4754,  0.5011, -0.7025,\n",
      "         1.6934,  1.5623,  0.0175,  0.3755,  0.4631, -0.2384,  0.7147,  0.0527,\n",
      "        -1.1090, -1.7358,  1.5149,  1.5359, -0.0177,  0.3222, -1.1168,  0.5889,\n",
      "        -1.2451, -0.6526,  1.7656,  2.3011, -1.8702, -0.7264,  2.4680, -2.0532,\n",
      "        -0.6996,  0.9588, -1.0180,  0.8967,  2.4436,  1.2629, -0.2996,  0.3502,\n",
      "         1.3303, -0.8304,  1.5775,  2.8758, -1.5843, -2.1395, -3.3604, -1.0547,\n",
      "        -1.3380, -0.4626, -0.5989,  1.1985,  1.1864, -1.5872, -0.4276, -0.6333,\n",
      "        -1.8529,  0.4926, -0.5662,  1.0274, -0.1375,  0.7809, -1.9124, -0.1382,\n",
      "         0.4262,  1.6580, -1.0205, -0.2895,  0.2009, -0.5947,  0.3367,  1.1363,\n",
      "        -0.8693,  1.7772,  0.6368, -0.2317,  1.6827,  0.5646, -1.0460, -0.6490,\n",
      "         0.3755, -0.0827, -0.3476, -0.0235,  1.6043, -0.1463, -0.1351, -0.6796,\n",
      "        -0.5453,  1.3104,  1.7361,  1.6748,  2.9928,  2.2312,  0.2277,  2.1114,\n",
      "        -0.1652,  0.5528, -0.0599, -0.0839, -0.4131,  2.1976,  0.7329, -0.2708,\n",
      "        -0.4057,  1.7361,  0.1265, -1.0222, -1.1307, -2.2754, -0.5421,  1.6590,\n",
      "         1.9663, -1.0383, -0.4460, -1.4358, -0.2096,  2.8020, -2.2303, -0.1797,\n",
      "        -0.2920, -0.2762,  0.3009, -0.1385, -0.1233,  0.8683, -1.5413,  1.8042,\n",
      "         0.9637,  1.4644,  1.1457, -0.1736,  0.3466,  0.9372,  0.2103, -1.0217,\n",
      "         1.6476,  1.8882, -0.3479, -1.3768,  0.4711,  0.9735, -0.4651, -1.2981,\n",
      "         0.9221, -0.8164, -0.7142,  0.3494,  0.4475, -1.0165, -0.7673,  0.2592,\n",
      "         0.0264,  0.6812,  1.6130,  1.4674,  0.7689, -1.1739,  1.7922,  1.9357,\n",
      "         1.5666, -2.7309,  0.8212, -0.3993, -1.3876, -0.9140,  1.1925,  1.9473,\n",
      "         0.7819, -1.2680, -1.7562,  0.0433, -1.7185, -0.8815, -0.9586, -0.2872,\n",
      "        -0.1496,  2.7114,  0.7239, -0.3988, -0.1776,  1.9768,  4.0460,  0.1638,\n",
      "         0.6799,  1.7810,  0.9107,  1.1526, -0.7735,  0.0831, -0.1786,  1.1976,\n",
      "        -0.1720,  1.8850,  0.3063,  0.4580, -0.6543, -0.1753, -3.3243, -2.9400,\n",
      "         1.2109, -1.6884,  0.5261, -0.3219, -0.9457,  0.2774, -0.5267, -0.2573,\n",
      "        -1.0684, -0.2480,  0.4862, -1.1067,  0.3915,  1.5942, -0.9297, -1.3709,\n",
      "         0.7943,  0.6650, -2.2717, -2.5066,  1.7469, -0.1709, -2.4652, -0.2627,\n",
      "         2.4768,  0.6930,  1.2288, -0.2472,  0.0316,  0.3429,  1.4521,  1.1226,\n",
      "         1.3034, -0.7404, -0.5331, -0.6002,  1.6782, -0.8647,  0.3454,  0.2443,\n",
      "         0.5868,  2.1485, -0.2210,  0.8215,  1.0593, -0.2713,  1.3056,  2.6451,\n",
      "         0.7721,  1.0777, -1.1603,  1.0359,  0.3833, -0.8933, -0.4875,  0.0676,\n",
      "         1.2505, -1.4334,  1.3211,  0.0570,  0.7179, -1.8299,  0.5675,  0.8264,\n",
      "        -2.2099,  0.1262, -0.3237,  0.4813, -1.5144, -1.3646,  0.8214, -0.0948,\n",
      "        -0.4089,  2.8052, -0.0216, -0.5406,  1.2069, -2.0983, -0.3172,  0.5497,\n",
      "        -0.5875, -2.1950, -0.1127, -1.7203, -0.4850, -1.0847,  0.3032, -0.2607,\n",
      "        -0.9734, -0.3338,  1.7121, -2.0342, -0.2369, -0.2629, -1.4330, -2.3595,\n",
      "        -1.3758,  1.5091,  0.6013, -1.2556,  0.9698,  1.5947, -0.3982,  1.8219,\n",
      "         0.5200, -0.5188,  0.1445,  0.3871,  1.0214,  0.9569, -1.5704,  1.2074,\n",
      "        -0.0932,  1.7708, -0.1301, -0.6940,  0.3509, -2.0404,  2.8369, -0.0488,\n",
      "         1.4691, -1.6739, -0.7375, -0.1919, -1.4216, -1.6117,  0.0346, -1.2351],\n",
      "       requires_grad=True), tensor([ 2.9913e-01, -9.2390e-01, -2.7255e-02,  1.1391e+00,  6.0517e-01,\n",
      "         2.9814e-01,  9.5503e-01, -9.4139e-02, -1.4407e+00,  1.9634e+00,\n",
      "        -4.7382e-01, -4.8296e-01,  4.9434e-01,  1.6896e+00,  3.0070e-01,\n",
      "         2.2033e-01,  1.5727e-01, -3.3270e+00,  7.3935e-01, -6.5704e-01,\n",
      "         1.1812e+00, -3.8536e+00,  1.1788e+00,  2.6455e-01, -4.4640e+00,\n",
      "        -3.2568e-01, -1.5055e+00,  1.4235e+00,  2.0938e+00,  2.7624e-01,\n",
      "         1.4806e+00,  1.3665e+00,  1.8999e+00,  2.2367e+00,  1.5362e+00,\n",
      "        -5.7587e-01,  1.7222e-01,  1.2653e+00,  2.0669e-02,  1.7652e+00,\n",
      "         1.3262e-01,  1.3863e+00,  2.0224e+00,  1.1027e+00, -1.6188e+00,\n",
      "         9.2918e-01, -1.6868e+00,  1.6381e+00, -1.7494e+00,  3.7387e-01,\n",
      "         1.4783e+00, -2.0153e+00,  7.8978e-01,  3.2773e+00, -8.1025e-01,\n",
      "         1.5068e+00, -1.0631e+00,  6.4393e-03, -1.1853e-01,  1.7348e-01,\n",
      "        -1.0127e+00,  1.6434e+00, -6.0514e-02,  1.4575e+00, -1.4784e+00,\n",
      "         7.1703e-02,  1.8057e+00,  1.2159e-01,  1.1484e+00, -7.8892e-01,\n",
      "         9.3573e-01, -2.4852e-01,  2.1999e+00, -2.3109e-01, -2.3171e-01,\n",
      "         5.9934e-01, -2.2902e+00, -3.8600e-01,  1.3206e+00,  1.3548e+00,\n",
      "         2.6772e-01, -8.7972e-01, -1.5372e+00,  1.1747e+00, -2.7174e+00,\n",
      "        -1.1489e+00, -9.0962e-01, -1.9440e-01,  1.5773e+00, -1.6382e+00,\n",
      "         2.8887e+00, -2.2456e+00,  7.6399e-02,  1.6787e+00,  5.7009e-02,\n",
      "        -7.4628e-01,  3.4156e-01,  2.0604e-01,  2.1939e-01,  2.3052e+00,\n",
      "        -2.3680e+00,  1.4196e-01, -3.5372e-01,  1.0721e+00, -1.1940e-01,\n",
      "         8.7561e-01,  1.6132e+00, -1.0069e-01, -3.6517e+00, -3.3001e+00,\n",
      "        -2.8742e-01, -1.7441e+00,  2.3151e+00, -8.1929e-01,  2.1464e+00,\n",
      "        -2.6642e+00,  1.4040e+00,  6.4563e-01,  2.1100e+00,  6.5146e-01,\n",
      "        -3.9177e-01, -5.3967e-01,  2.8399e+00,  1.4673e+00, -1.5305e+00,\n",
      "        -5.0504e-01, -4.6021e+00,  1.6169e+00, -1.2747e+00,  1.8730e+00,\n",
      "         9.2038e-01,  3.7693e+00,  1.2790e+00,  2.3817e+00,  4.9867e-01,\n",
      "        -2.7475e+00, -2.4327e-01,  3.6646e-02, -7.2124e-01, -2.6668e-01,\n",
      "        -1.3723e-01,  1.2314e+00, -1.2402e+00, -1.5148e+00, -2.9013e-01,\n",
      "        -2.7127e+00, -3.2265e-01,  2.6758e-01, -4.5864e-01,  3.8807e-01,\n",
      "        -2.4482e+00,  6.6670e-01,  1.0316e+00,  2.9598e-01, -8.7809e-01,\n",
      "         1.7267e+00,  1.2180e+00, -4.7216e-03,  2.3133e+00,  6.2573e-01,\n",
      "         1.3168e+00, -2.0968e+00, -1.4124e+00,  3.9283e-01, -2.9417e-01,\n",
      "        -2.8912e-01, -2.0062e+00,  2.0937e+00,  6.9813e-01,  4.1082e-01,\n",
      "        -8.4570e-01, -1.4648e+00, -3.1430e+00, -7.5408e-01, -4.2862e-01,\n",
      "        -1.8073e+00, -1.7019e-01,  5.3665e-01,  1.1691e-02, -9.7667e-01,\n",
      "         8.9561e-01, -8.4160e-01, -1.9225e-02,  5.9038e-01, -1.7659e-01,\n",
      "         3.4544e-01, -1.9671e+00, -1.1137e+00,  4.0886e+00, -8.2499e-01,\n",
      "        -1.2876e+00,  2.5245e-01, -4.0598e+00,  8.1659e-01, -9.8534e-01,\n",
      "         9.9731e-01, -1.8965e+00,  1.6072e+00, -1.7001e+00,  6.0911e-01,\n",
      "        -1.2056e+00,  6.5214e-01, -2.1555e+00,  7.8953e-01, -1.2602e+00,\n",
      "         2.5992e+00,  1.5913e+00,  1.6392e-01,  3.9419e-01,  4.3795e-01,\n",
      "         1.8470e+00, -1.4120e-01, -1.0153e+00, -9.0356e-01,  1.2097e+00,\n",
      "        -1.1924e-01,  5.6482e-01,  2.4288e-01, -1.2665e+00,  2.7680e+00,\n",
      "        -5.5808e-01, -1.7606e-01,  2.4779e+00, -1.5682e+00,  4.7802e-01,\n",
      "         9.2141e-01, -2.7110e+00, -1.3607e+00,  1.5657e+00,  2.8998e+00,\n",
      "        -2.1353e+00,  1.4464e+00, -5.9548e+00,  2.6630e+00, -4.3910e-01,\n",
      "        -3.1314e-01, -3.1517e+00,  1.7917e+00,  1.4366e+00,  2.0223e+00,\n",
      "        -1.2098e+00, -8.0730e-02,  9.3023e-01,  1.0265e+00,  2.2044e+00,\n",
      "         4.0740e-01, -1.1783e+00, -5.9877e-01,  2.8796e-01, -1.1635e+00,\n",
      "         3.1357e+00, -2.3676e+00, -1.2678e-01,  7.6563e-01, -2.2534e+00,\n",
      "        -7.6935e-01,  1.8223e+00, -2.5265e+00, -1.3084e-01,  1.1012e+00,\n",
      "        -8.6610e-01, -1.2935e+00,  2.0050e+00,  1.5978e-01, -2.9219e-01,\n",
      "        -1.6043e+00, -2.2330e+00, -2.3133e+00, -2.2980e+00,  1.3201e+00,\n",
      "         2.0950e+00,  8.3501e-01, -2.2475e-01,  1.7096e+00,  2.3724e+00,\n",
      "         3.4165e+00,  2.1512e-01,  3.0464e-01,  1.9094e+00,  3.5005e+00,\n",
      "         8.8786e-01,  9.9864e-01, -1.7076e+00,  9.3109e-01, -6.3577e-01,\n",
      "         2.0288e+00,  9.3434e-01, -2.0836e-01, -8.7581e-01,  5.0574e-01,\n",
      "         8.3667e-01, -4.6744e-01, -5.8909e-01, -8.7489e-01,  1.6275e+00,\n",
      "        -8.8101e-01, -1.0175e+00,  9.2811e-01,  1.0512e+00,  1.1583e+00,\n",
      "         1.6731e+00, -8.9489e-01,  2.3805e+00, -1.5308e+00,  5.0185e-01,\n",
      "         1.4234e+00, -3.2742e+00,  1.3393e+00, -8.9075e-01,  2.6865e+00,\n",
      "        -8.9263e-01, -3.7384e-01, -2.8495e+00, -1.0194e+00,  7.1135e-01,\n",
      "         3.9411e-01, -2.0167e-01,  1.6008e-01, -2.6869e-01,  2.4658e+00,\n",
      "         1.4565e+00, -6.1870e-01,  1.9090e+00,  8.2847e-02, -1.0659e+00,\n",
      "        -2.5524e+00, -7.6382e-01,  3.1893e-01,  7.3026e-02,  1.4086e+00,\n",
      "        -1.1549e+00, -1.7633e-01,  9.9080e-01, -1.0316e+00,  2.8902e-01,\n",
      "        -2.1370e-01, -1.0154e-02, -3.3396e+00,  7.9630e-01, -2.6184e+00,\n",
      "         5.1547e-01, -9.5755e-03, -1.0412e-01, -1.3618e-01,  8.7361e-01,\n",
      "         1.9106e+00,  2.1698e+00,  7.9099e-01,  1.3498e+00, -1.3750e+00,\n",
      "        -2.9532e+00, -6.4805e-01, -2.0029e+00, -1.2619e-01,  9.6127e-02,\n",
      "         1.8273e+00, -4.1432e-01, -2.9868e+00, -7.6700e-01,  1.7009e+00,\n",
      "         2.3324e+00, -3.6758e-01, -1.3086e+00, -6.4721e-01, -1.7694e+00,\n",
      "         7.3455e-01,  6.1240e-01, -5.2322e-01,  6.4363e-01, -1.9032e+00,\n",
      "         1.2105e+00, -2.2813e-01, -1.8650e+00, -2.4776e+00, -1.1938e+00,\n",
      "        -1.2052e+00,  9.8449e-01,  9.7935e-01,  2.4342e+00, -2.7992e+00,\n",
      "        -3.1685e-01,  1.7514e+00, -1.0649e+00, -9.5293e-01,  3.3222e-01,\n",
      "         9.4205e-01,  1.0252e-01, -1.9473e+00, -1.3527e+00,  9.5848e-01,\n",
      "         7.4143e-01,  2.2408e+00,  4.0233e-02, -9.2550e-01, -6.3651e-01,\n",
      "        -2.9572e+00, -9.2786e-01,  1.4079e+00,  3.4428e+00, -5.0984e-02,\n",
      "         9.0471e-01, -2.0093e-01, -1.6978e+00,  1.4443e+00,  1.9215e+00,\n",
      "         1.2572e+00, -1.2774e+00, -1.2891e+00, -1.3625e+00, -1.2520e+00,\n",
      "         5.6843e-01, -7.0358e-01,  1.8333e+00, -1.4174e+00,  7.5321e-01,\n",
      "         5.1465e-01, -6.6472e-01,  1.0325e+00,  3.5726e-01, -2.9357e+00,\n",
      "         6.8124e-01,  2.2773e-01,  2.1723e+00, -4.5229e-02,  1.5275e-01,\n",
      "        -9.0381e-01,  3.0579e+00,  2.7129e+00,  9.6637e-01, -9.9866e-01,\n",
      "        -1.8506e+00,  4.9871e-01, -7.6983e-01,  2.6577e+00, -1.0122e-01,\n",
      "         1.9971e+00,  1.0532e+00,  1.4487e+00,  1.2616e+00, -6.2757e-01,\n",
      "        -2.0925e+00,  1.9634e-02,  1.5744e+00, -4.8379e-01,  7.9768e-02,\n",
      "        -2.2635e+00, -1.9785e+00,  1.4565e+00,  9.7845e-01, -6.3830e-01,\n",
      "        -2.4472e+00, -1.9666e+00, -6.1187e-02,  1.7370e+00, -9.0761e-01,\n",
      "        -2.0232e-02, -2.1898e-01, -1.5224e+00,  1.3639e+00,  1.9397e-01,\n",
      "        -3.7581e-01,  8.9133e-01,  1.7978e+00,  9.2062e-01,  2.3712e-01,\n",
      "        -5.2346e-01,  2.1575e-01,  3.9676e+00, -2.5856e+00,  2.7457e+00,\n",
      "        -1.6137e+00,  5.8762e-01,  5.5334e-01,  1.2910e+00, -1.7027e+00,\n",
      "        -3.5880e+00, -2.5391e+00,  1.0823e+00,  1.1524e+00,  8.1649e-02,\n",
      "        -3.5000e-01,  2.2977e+00,  2.4110e+00, -1.2123e+00, -2.3289e-01,\n",
      "         5.0541e-02,  2.1055e-01, -2.3309e-01,  3.6792e-01,  3.3075e+00,\n",
      "         2.6917e+00,  8.0831e-01, -7.7904e-01, -2.2808e+00, -5.8544e-01,\n",
      "        -3.5519e+00, -1.2700e+00,  2.6896e-01,  1.5210e+00, -2.4376e+00,\n",
      "        -4.0919e-01,  3.9450e-01, -4.5573e-01, -1.7134e+00,  6.9647e-01,\n",
      "        -2.4584e-01, -3.8255e-02, -5.7863e-01,  1.9929e+00, -3.2103e+00,\n",
      "         2.8375e-01,  6.6654e-01, -2.1397e+00,  1.1033e+00, -1.7986e+00,\n",
      "         1.2560e+00,  4.6298e-01, -4.6801e-01, -5.5934e-01, -4.5583e-02,\n",
      "         3.7324e-01, -3.4479e+00,  8.1969e-01, -7.2812e-02, -1.6164e+00,\n",
      "        -5.4805e-01, -5.5491e-01, -8.6933e-02,  1.1132e+00,  1.0031e+00,\n",
      "        -9.6285e-01,  3.4268e-01, -3.8773e-02,  3.3703e-01, -2.8004e-01,\n",
      "        -8.7970e-01,  7.4033e-01, -1.1367e+00, -3.0836e+00, -2.2656e+00,\n",
      "         7.1467e-01, -1.1129e-02, -1.3780e+00,  1.3326e+00, -1.1580e+00,\n",
      "         7.8689e-01, -2.5636e+00, -3.0634e-01,  1.4024e+00,  4.9321e-01,\n",
      "        -8.3398e-01,  9.9190e-01,  1.3489e+00, -9.9413e-01,  1.7289e+00,\n",
      "        -6.8805e-01,  9.4775e-01,  3.4045e-01,  6.3480e-01, -9.3823e-01,\n",
      "         2.4148e+00, -7.8586e-01,  2.4690e+00,  1.7771e+00, -2.8554e+00,\n",
      "        -5.1189e-01,  2.3898e+00,  1.4938e+00, -1.0856e+00,  8.9855e-02,\n",
      "         2.4111e+00,  8.8723e-01,  4.4409e-01,  2.2942e+00,  4.9858e-01,\n",
      "         4.7714e-01,  4.8516e-01,  2.1664e+00,  3.2181e-02, -1.5185e+00,\n",
      "        -7.3029e-02,  1.1094e+00,  4.5892e-01,  1.2711e-01,  1.5373e+00,\n",
      "        -2.2752e+00,  2.8794e+00,  1.3795e+00, -2.0699e+00,  1.3169e+00,\n",
      "        -1.1069e+00,  1.0238e-01, -1.7535e+00,  7.5390e-01,  1.3221e+00,\n",
      "        -6.4385e-01, -2.9343e+00, -9.9499e-03,  3.3710e-02,  4.0600e-01,\n",
      "         9.8408e-01, -2.5789e+00,  1.0481e+00,  2.8984e-01, -2.3753e+00,\n",
      "        -1.8286e+00,  1.5541e+00,  1.5529e+00,  1.2219e+00, -2.1854e-01,\n",
      "        -2.5863e+00, -1.0968e+00, -3.8893e-01, -2.9255e+00, -1.0405e+00,\n",
      "         1.9607e+00, -2.2779e-01,  3.9726e-01, -1.0341e+00, -8.0977e-01,\n",
      "         7.0834e-01, -1.3897e+00,  3.2922e-01, -2.7019e+00,  1.5181e+00,\n",
      "         1.0490e+00, -1.6596e+00, -1.4582e+00, -2.5758e+00, -9.7150e-01,\n",
      "        -2.5973e+00,  1.1111e+00,  8.3700e-01,  3.7234e-01,  1.0870e+00,\n",
      "        -2.7248e-01, -4.5344e-01,  1.7633e+00, -1.7090e+00,  1.6220e+00,\n",
      "         1.4628e+00,  1.7665e-01, -2.1406e+00,  5.2464e-01, -1.1736e-01,\n",
      "         2.0161e+00, -6.9831e-01, -1.9940e+00,  1.2840e+00,  1.0612e+00,\n",
      "         8.5984e-01, -1.3691e+00,  2.5914e+00, -2.5506e+00, -1.2234e-01,\n",
      "        -2.9992e+00,  1.8989e+00, -1.2599e+00, -5.9914e-02, -2.6256e+00,\n",
      "        -1.9749e-01, -8.3943e-01, -1.3928e+00, -3.9592e-01,  1.0521e+00,\n",
      "         9.5598e-02,  9.8548e-01, -1.3159e+00, -5.9487e-01, -5.5096e-01,\n",
      "        -1.2743e+00, -1.2881e+00, -5.9235e-01, -1.8104e-01, -1.8188e+00,\n",
      "         2.0187e-01,  1.6425e+00, -4.5413e-01, -2.5320e+00,  4.5581e-01,\n",
      "         1.9903e+00, -1.5041e+00,  6.5774e-02, -7.9731e-01, -8.2017e-01,\n",
      "         4.8316e-01,  1.3267e+00,  2.9330e+00, -1.0895e-01,  1.5932e+00,\n",
      "        -4.9002e-01, -1.3496e+00,  8.1242e-01, -4.3838e+00, -7.2984e-01,\n",
      "        -1.1379e+00,  1.6517e+00, -2.1245e+00,  3.9326e+00, -7.0234e-01,\n",
      "        -9.1476e-01, -1.5455e+00,  1.3474e+00, -2.2956e+00,  1.6631e+00,\n",
      "        -5.9315e-01, -1.5186e+00,  4.1386e-01,  1.9141e+00,  1.5178e+00,\n",
      "        -1.9655e+00,  1.1807e+00, -1.7276e+00, -2.6727e+00,  1.7875e+00,\n",
      "        -1.5573e+00, -1.4228e-02, -1.5365e+00,  4.8404e-01,  1.0438e+00,\n",
      "        -3.9699e-01,  8.3233e-01, -1.4228e+00,  7.8535e-02, -5.2736e-01,\n",
      "         1.3401e-01,  3.7784e-01,  2.4666e+00, -1.8996e+00, -7.7248e-01,\n",
      "         1.0540e-01,  4.2098e-01, -2.2589e-01, -2.1383e+00,  6.9498e-02,\n",
      "        -1.9158e+00, -2.0305e+00, -3.0252e+00, -5.5714e-01,  5.6874e-01,\n",
      "        -5.3498e-01,  1.6587e+00,  1.1719e+00,  3.9189e+00, -7.5784e-01,\n",
      "         7.6097e-01, -3.6902e-01,  8.4838e-01,  7.9243e-01,  4.3095e-01,\n",
      "        -5.3602e-01, -2.0851e-02, -2.1990e+00,  6.8199e-01,  1.3164e+00,\n",
      "        -3.7081e-01,  1.0276e+00,  1.7998e+00, -4.8237e-01,  1.3842e+00,\n",
      "         5.6788e-01, -1.2557e+00, -9.0693e-01,  1.2155e+00, -2.8057e+00,\n",
      "        -1.3625e+00,  1.5703e+00,  2.3572e+00], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print( f\"The embedding for the sentence {sentence_to_produce} is \\n{learned_magic_token_list}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a972c95b-dce0-4359-8d62-754bd3f2f86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
