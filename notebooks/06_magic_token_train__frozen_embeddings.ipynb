{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be9a8a0-3812-468e-8e76-14474518a0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from accelerate) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from accelerate) (1.25.2)\n",
      "Requirement already satisfied: psutil in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.0.1)\n",
      "Requirement already satisfied: networkx in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.101)\n",
      "Requirement already satisfied: jinja2 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.2.10.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.14.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.5.0.96)\n",
      "Requirement already satisfied: typing-extensions in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.7.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.4.91)\n",
      "Requirement already satisfied: filelock in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.3)\n",
      "Requirement already satisfied: sympy in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (59.6.0)\n",
      "Requirement already satisfied: wheel in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate) (0.41.2)\n",
      "Requirement already satisfied: cmake in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.4.1)\n",
      "Requirement already satisfied: lit in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: transformers[torch] in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (4.33.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (2023.8.8)\n",
      "Requirement already satisfied: requests in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (0.17.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (1.25.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (0.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: filelock in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (3.12.3)\n",
      "Requirement already satisfied: torch!=1.12.0,>=1.10 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: accelerate>=0.20.3 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from transformers[torch]) (0.22.0)\n",
      "Requirement already satisfied: psutil in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers[torch]) (4.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.14.3)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.0.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.10.3.66)\n",
      "Requirement already satisfied: networkx in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: sympy in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.7.91)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.7.101)\n",
      "Requirement already satisfied: jinja2 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.4.0.1)\n",
      "Requirement already satisfied: setuptools in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.10->transformers[torch]) (59.6.0)\n",
      "Requirement already satisfied: wheel in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch!=1.12.0,>=1.10->transformers[torch]) (0.41.2)\n",
      "Requirement already satisfied: cmake in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (3.27.4.1)\n",
      "Requirement already satisfied: lit in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from triton==2.0.0->torch!=1.12.0,>=1.10->transformers[torch]) (16.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from requests->transformers[torch]) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate -U\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99aa81de-8944-4dd4-98c0-b2d00a451f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle, torch\n",
    "from ipywidgets import Dropdown\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3175f0ed-b1b8-4713-8fc2-5bca6a7c3e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get a list of the translations available for training.\n",
    "trained_model_folder = \"./out\"\n",
    "language_pair_staging_folder = \"../data/magic_token_folder/\"\n",
    "use_side_frozen_tokens = True\n",
    "if use_side_frozen_tokens:\n",
    "    num_magic_tokens = 1\n",
    "    trained_embeddings_pickle_file = os.path.join( language_pair_staging_folder, f\"magic_tokens_size_{num_magic_tokens}.pickle\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd26a6-742c-48ec-a851-074a5b78f076",
   "metadata": {},
   "source": [
    " - Primary models end with _model.\n",
    " - They are trained and then saved out as _model_step.\n",
    " - The embedding syncer averages together all the _model_step files and save them individually back as _model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9df5aef-c41c-420e-99a4-597cefdbf546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders ending with '_model' or '_step': ['hebrew_model', 'target_model', 'hebrew_model_step', 'greek_model', 'bsb_model']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(trained_model_folder) and os.path.isdir(trained_model_folder):\n",
    "    # Get a list of all subfolders in language_pair_staging_folder\n",
    "    subfolders = [folder for folder in os.listdir(trained_model_folder) if os.path.isdir(os.path.join(trained_model_folder, folder))]\n",
    "\n",
    "    # Filter subfolders that end with \"_model\"\n",
    "    model_folders = [folder for folder in subfolders if folder.endswith(\"_model\") or folder.endswith( \"_model_step\" ) ]\n",
    "\n",
    "    # Print or use the list of model folders\n",
    "    print(\"Folders ending with '_model' or '_step':\", model_folders)\n",
    "else:\n",
    "    print(f\"The folder '{trained_model_folder}' does not exist or is not a directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b01d87b-4976-4961-b649-0f583a42fba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select which model to train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be62d1a65b0449c9a7e113670b7936f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(options=('hebrew_model', 'target_model', 'hebrew_model_step', 'greek_model', 'bsb_model'), value='heb…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "selected_model_dropdown = Dropdown(options=model_folders)\n",
    "print( \"Select which model to train\" )\n",
    "display(selected_model_dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3246b2e8-21e9-4d98-b133-569fa3267eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded previouse training from ../data/magic_token_folder/magic_tokens_size_1.pickle\n"
     ]
    }
   ],
   "source": [
    "#training code copied from\n",
    "#https://www.kaggle.com/code/changyeop/how-to-fine-tune-gpt-2-for-beginners/notebook\n",
    "\n",
    "def load_dataset(file_path, tokenizer, block_size = 128):\n",
    "    dataset = TextDataset(\n",
    "        tokenizer = tokenizer,\n",
    "        file_path = file_path,\n",
    "        block_size = block_size,\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "def load_data_collator(tokenizer, mlm = False):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, \n",
    "        mlm=mlm,\n",
    "    )\n",
    "    return data_collator\n",
    "\n",
    "\n",
    "if use_side_frozen_tokens:\n",
    "    learned_magic_tokens = {}\n",
    "    if os.path.exists(trained_embeddings_pickle_file):\n",
    "        with open(trained_embeddings_pickle_file, 'rb') as file:\n",
    "            learned_magic_tokens = pickle.load(file)\n",
    "            print( f\"Loaded previouse training from {trained_embeddings_pickle_file}\" )\n",
    "    else:\n",
    "        print( f\"Previouse training not found at {trained_embeddings_pickle_file}\" )\n",
    "    \n",
    "    class MagicTokensEmbeddingPatch(nn.Module):\n",
    "        def __init__(self, tokenizer, other_embeddings ):\n",
    "            super().__init__()\n",
    "            self.tokenizer = tokenizer\n",
    "            self.other_embeddings = other_embeddings\n",
    "    \n",
    "        def forward( self, x ):\n",
    "            #print( f\"Called forward with x {x}\" )\n",
    "            batch_result_list = []\n",
    "            for batch in x:\n",
    "                #print( f\"batch is {batch}\" )\n",
    "                inputs_embeds_list = []\n",
    "                for id in batch:\n",
    "                    #print( f\"id is {id}\" )\n",
    "                    token_string = self.tokenizer.decode( [id] )\n",
    "                    #print( f\"token_string is {token_string}\" )\n",
    "                    if token_string.startswith( '[' ) and token_string.endswith( ']' ):\n",
    "                        if token_string in learned_magic_tokens:\n",
    "                            inputs_embeds_list.append( learned_magic_tokens[token_string].detach() )\n",
    "                        else:\n",
    "                            #can't drop stuff because it makes the batches not be the same length.\n",
    "                            #print( f\"Dropped {token_string}\" )\n",
    "                            inputs_embeds_list.append( self.other_embeddings( torch.LongTensor(tokenizer.encode(\"_\")).to(x.device) )[0] )\n",
    "                    else:\n",
    "                        inputs_embeds_list.append( self.other_embeddings( torch.LongTensor([id]).to(x.device) )[0] )\n",
    "                        \n",
    "                inputs_embeds = torch.stack(inputs_embeds_list, dim=0)\n",
    "                batch_result_list.append(inputs_embeds)\n",
    "            return torch.stack(batch_result_list, dim=0)\n",
    "\n",
    "def train(train_file_path,model_name,\n",
    "          output_dir,\n",
    "          overwrite_output_dir,\n",
    "          per_device_train_batch_size,\n",
    "          num_train_epochs,\n",
    "          save_steps,\n",
    "          tokenizer=None):\n",
    "    if tokenizer is None:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    train_dataset = load_dataset(train_file_path, tokenizer)\n",
    "    data_collator = load_data_collator(tokenizer)\n",
    "    \n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "      \n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    \n",
    "    model.save_pretrained(output_dir)\n",
    "\n",
    "    if use_side_frozen_tokens:\n",
    "        model.set_input_embeddings( MagicTokensEmbeddingPatch(tokenizer,model.get_input_embeddings()) )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "          output_dir=output_dir,\n",
    "          overwrite_output_dir=overwrite_output_dir,\n",
    "          per_device_train_batch_size=per_device_train_batch_size,\n",
    "          num_train_epochs=num_train_epochs,\n",
    "          save_total_limit=4,\n",
    "      )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "          model=model,\n",
    "          args=training_args,\n",
    "          data_collator=data_collator,\n",
    "          train_dataset=train_dataset,\n",
    "    )\n",
    "    \n",
    "    # if only_train_magic_tokens:\n",
    "    #     #freeze everything\n",
    "    #     for param in model.parameters():\n",
    "    #       param.requires_grad = False\n",
    "        \n",
    "    #     #Thaw the magic parameters.\n",
    "    #     first_magic_token_ids = tokenizer.encode( \"[GEN 1:1_a]\" )\n",
    "    #     assert (len(first_magic_token_ids) == 1)\n",
    "    #     first_magic_token_id = first_magic_token_ids[0]\n",
    "\n",
    "    #     #assume that the magic tokens are all rest of the tokens.\n",
    "    #     for token_id in range(first_magic_token_id,len(tokenizer)):\n",
    "    #         model.get_input_embeddings().weight[token_id].requires_grad = True\n",
    "    #         model.get_output_embeddings().weight[token_id].requires_grad = True\n",
    "        \n",
    "      \n",
    "    trainer.train()\n",
    "    trainer.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "550803ce-cd5d-4c80-b258-bbe2c2bb389f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'greek'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_model_dropdown.value\n",
    "target_model = selected_model_dropdown.value.replace( \"_model\", \"\" ).replace( \"_step\", \"\" )\n",
    "target_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea640b78-f77b-40d9-a79b-9138ea2764f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you need to set parameters \n",
    "train_file_path = os.path.join( language_pair_staging_folder, f\"train_{target_model}.txt\" )\n",
    "model_name = os.path.join( trained_model_folder, selected_model_dropdown.value )\n",
    "output_dir = os.path.join( trained_model_folder, f\"{target_model}_model_step\" )\n",
    "overwrite_output_dir = True\n",
    "per_device_train_batch_size = 2 #8\n",
    "num_train_epochs = 5.0\n",
    "save_steps = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4408ce0-6a25-4b7c-9cf4-213f68c35aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6350657d-f26c-41e2-908a-626ff75ae285",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f8dcce-58f0-4d6d-88f0-2c73f355fbe0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed47557b-06ca-47d9-a669-bd52ba60a5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba53574a-6f03-4986-8ecd-b9959e68a997",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21f10acd-1588-44ba-9cb4-6b119143a1bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./out/hebrew_model_step'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b67f3b30-9e12-43da-ad32-56fda7016332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lansford/projects/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='23330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   13/23330 00:13 < 7:42:33, 0.84 it/s, Epoch 0.00/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_file_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverwrite_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_train_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 111\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(train_file_path, model_name, output_dir, overwrite_output_dir, per_device_train_batch_size, num_train_epochs, save_steps, tokenizer)\u001b[0m\n\u001b[1;32m     88\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     89\u001b[0m       model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     90\u001b[0m       args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     91\u001b[0m       data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[1;32m     92\u001b[0m       train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     93\u001b[0m )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# if only_train_magic_tokens:\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m#     #freeze everything\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;66;03m#     for param in model.parameters():\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m#         model.get_input_embeddings().weight[token_id].requires_grad = True\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m#         model.get_output_embeddings().weight[token_id].requires_grad = True\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n",
      "File \u001b[0;32m~/projects/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/transformers/trainer.py:1553\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/transformers/trainer.py:1835\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1834\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1835\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1838\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1839\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1840\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1841\u001b[0m ):\n\u001b[1;32m   1842\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/projects/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/transformers/trainer.py:2690\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2688\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2689\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2690\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/projects/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/accelerate/accelerator.py:1923\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1923\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(\n",
    "    train_file_path=train_file_path,\n",
    "    model_name=model_name,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=overwrite_output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    save_steps=save_steps,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a267d0-c37a-4042-862b-e9f169f27461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c778703f-f942-4a5e-9d78-5312ad970340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model greek_model to ./out/greek_model_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23330' max='23330' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23330/23330 59:40, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.137500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.651000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.546300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.466200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.378300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.357700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.213200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.203800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.191600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.168100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.155600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.126600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.130800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.104500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.097500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.103400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.096600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.049900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.057000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.061000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.063800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.069800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.062100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.049500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.055300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.064100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.032800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>1.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>1.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>1.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>1.017800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>1.021500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>1.018700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>1.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>1.020300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model bsb_model to ./out/bsb_model_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19050' max='19050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19050/19050 48:12, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.295000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.405400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.281600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.247800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>2.088800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.853800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.872600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.867300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.888100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.895100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.923900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.931600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.700300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.662300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.734800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>1.733300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>1.791800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>1.780400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>1.570700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>1.597100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>1.636200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>1.642200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>1.686000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>1.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>1.722900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>1.671800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>1.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>1.605100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>1.659600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>1.668200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>1.708400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>1.715900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>1.731400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model target_model to ./out/target_model_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7825' max='7825' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7825/7825 20:05, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.118300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.006400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>3.722600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.666900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>3.540700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.513100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>3.480200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>3.348100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>3.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.279900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>3.287100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model hebrew_model to ./out/hebrew_model_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lansford/Sync/projects/tf_over/hackathon/microsoft_gpt_hackathon_2023/sil-microsoft-hackathon-2023/venv/lib/python3.10/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='86545' max='86545' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [86545/86545 3:55:22, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.783800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.805100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.809300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.824800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.821800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.817100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.816200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.812800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.817100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.810400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.828500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.810500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.821200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.809400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.821900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.808100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.806800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.801600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>0.813700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.805100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>0.803900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.807100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>0.809800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.808800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14500</td>\n",
       "      <td>0.798200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.798900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15500</td>\n",
       "      <td>0.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.792700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16500</td>\n",
       "      <td>0.809700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.812700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17500</td>\n",
       "      <td>0.794300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.762300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18500</td>\n",
       "      <td>0.759700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.766000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19500</td>\n",
       "      <td>0.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.760300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20500</td>\n",
       "      <td>0.768900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.760100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21500</td>\n",
       "      <td>0.756100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.762100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22500</td>\n",
       "      <td>0.755400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23500</td>\n",
       "      <td>0.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.761700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24500</td>\n",
       "      <td>0.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.771300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25500</td>\n",
       "      <td>0.774500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.762600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26500</td>\n",
       "      <td>0.769700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.755900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27500</td>\n",
       "      <td>0.769600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28500</td>\n",
       "      <td>0.765100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.773900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29500</td>\n",
       "      <td>0.766100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.776300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30500</td>\n",
       "      <td>0.764000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>0.758200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31500</td>\n",
       "      <td>0.770100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>0.775000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32500</td>\n",
       "      <td>0.771200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>0.774100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33500</td>\n",
       "      <td>0.762800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>0.768600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34500</td>\n",
       "      <td>0.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>0.739700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35500</td>\n",
       "      <td>0.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>0.725900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36500</td>\n",
       "      <td>0.737800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>0.735100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37500</td>\n",
       "      <td>0.725100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>0.734100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38500</td>\n",
       "      <td>0.733900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>0.727600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39500</td>\n",
       "      <td>0.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>0.737700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40500</td>\n",
       "      <td>0.732400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>0.738800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41500</td>\n",
       "      <td>0.733200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>0.739100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42500</td>\n",
       "      <td>0.730900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43000</td>\n",
       "      <td>0.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43500</td>\n",
       "      <td>0.741700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44000</td>\n",
       "      <td>0.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44500</td>\n",
       "      <td>0.734700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45000</td>\n",
       "      <td>0.740000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45500</td>\n",
       "      <td>0.737100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46000</td>\n",
       "      <td>0.747500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46500</td>\n",
       "      <td>0.740600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47000</td>\n",
       "      <td>0.745400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47500</td>\n",
       "      <td>0.743700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48000</td>\n",
       "      <td>0.741300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48500</td>\n",
       "      <td>0.735300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49000</td>\n",
       "      <td>0.742100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49500</td>\n",
       "      <td>0.742800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50000</td>\n",
       "      <td>0.743400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50500</td>\n",
       "      <td>0.744900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51000</td>\n",
       "      <td>0.743300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51500</td>\n",
       "      <td>0.736000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52000</td>\n",
       "      <td>0.736300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52500</td>\n",
       "      <td>0.705100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53000</td>\n",
       "      <td>0.709100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53500</td>\n",
       "      <td>0.704600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54000</td>\n",
       "      <td>0.711900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54500</td>\n",
       "      <td>0.710300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55000</td>\n",
       "      <td>0.718800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55500</td>\n",
       "      <td>0.717800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56000</td>\n",
       "      <td>0.717800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56500</td>\n",
       "      <td>0.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57000</td>\n",
       "      <td>0.709400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57500</td>\n",
       "      <td>0.721000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58000</td>\n",
       "      <td>0.711900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58500</td>\n",
       "      <td>0.713600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59000</td>\n",
       "      <td>0.711400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59500</td>\n",
       "      <td>0.703900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60000</td>\n",
       "      <td>0.720100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60500</td>\n",
       "      <td>0.719900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61000</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>0.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>0.720900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>0.734000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63000</td>\n",
       "      <td>0.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63500</td>\n",
       "      <td>0.721700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64000</td>\n",
       "      <td>0.723300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64500</td>\n",
       "      <td>0.721100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65000</td>\n",
       "      <td>0.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65500</td>\n",
       "      <td>0.727900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66000</td>\n",
       "      <td>0.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66500</td>\n",
       "      <td>0.729000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67000</td>\n",
       "      <td>0.726400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67500</td>\n",
       "      <td>0.729500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68000</td>\n",
       "      <td>0.732300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68500</td>\n",
       "      <td>0.728300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69000</td>\n",
       "      <td>0.725400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69500</td>\n",
       "      <td>0.714500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70000</td>\n",
       "      <td>0.703200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70500</td>\n",
       "      <td>0.712500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71000</td>\n",
       "      <td>0.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71500</td>\n",
       "      <td>0.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72000</td>\n",
       "      <td>0.696500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72500</td>\n",
       "      <td>0.703000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73000</td>\n",
       "      <td>0.700400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73500</td>\n",
       "      <td>0.699600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74000</td>\n",
       "      <td>0.701400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74500</td>\n",
       "      <td>0.706500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75000</td>\n",
       "      <td>0.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75500</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76000</td>\n",
       "      <td>0.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76500</td>\n",
       "      <td>0.701700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77000</td>\n",
       "      <td>0.700700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77500</td>\n",
       "      <td>0.699500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78000</td>\n",
       "      <td>0.713700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78500</td>\n",
       "      <td>0.704700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79000</td>\n",
       "      <td>0.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79500</td>\n",
       "      <td>0.717300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80000</td>\n",
       "      <td>0.700600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80500</td>\n",
       "      <td>0.722700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81000</td>\n",
       "      <td>0.723300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81500</td>\n",
       "      <td>0.718700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82000</td>\n",
       "      <td>0.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82500</td>\n",
       "      <td>0.713900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83000</td>\n",
       "      <td>0.719400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83500</td>\n",
       "      <td>0.724200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84000</td>\n",
       "      <td>0.712600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84500</td>\n",
       "      <td>0.720200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85000</td>\n",
       "      <td>0.720300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85500</td>\n",
       "      <td>0.720800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86000</td>\n",
       "      <td>0.717900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86500</td>\n",
       "      <td>0.719400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Here we loop through and train all the non step models not just the selected one.\n",
    "# Filter subfolders that end with \"_model\"\n",
    "non_step_models = [folder for folder in subfolders if folder.endswith(\"_model\") ]\n",
    "\n",
    "for model_selection in non_step_models:\n",
    "    target_model = model_selection.replace( \"_model\", \"\" ).replace( \"_step\", \"\" )\n",
    "    train_file_path = os.path.join( language_pair_staging_folder, f\"train_{target_model}.txt\" )\n",
    "    model_name = os.path.join( trained_model_folder, selected_model_dropdown.value )\n",
    "    output_dir = os.path.join( trained_model_folder, f\"{target_model}_model_step\" )\n",
    "    print( f\"Training model {model_selection} to {output_dir}\" )\n",
    "    train(\n",
    "        train_file_path=train_file_path,\n",
    "        model_name=model_name,\n",
    "        output_dir=output_dir,\n",
    "        overwrite_output_dir=overwrite_output_dir,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        save_steps=save_steps\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea523f6-6487-4983-83dd-4c08b264211e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
