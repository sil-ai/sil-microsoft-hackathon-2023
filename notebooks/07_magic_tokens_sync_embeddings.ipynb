{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e74d78ef-5841-428f-97ab-ffaa836846ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, torch, random\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch.nn as nn\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baebb3ca-af29-46df-a864-f61974dffaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_folder = \"./out\"\n",
    "language_pair_staging_folder = \"../data/magic_token_folder/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1936c44f-603d-49cd-9e9b-711a6edf0002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    return model\n",
    "\n",
    "def load_tokenizer(tokenizer_path):\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(tokenizer_path)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b2fd7895-d9d9-4ed1-be3d-85e0bc7859d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_token_numbers( model_name, token = \"[GEN 1:1_a]\" ):\n",
    "    model_path = os.path.join( trained_model_folder, model_name )\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    model = load_model(model_path)\n",
    "    token_ids = tokenizer.encode( token )\n",
    "    assert (len(token_ids) == 1)\n",
    "    token_id = token_ids[0]\n",
    "    input_embeddings = model.get_input_embeddings()\n",
    "    token_embedding = input_embeddings(torch.LongTensor([token_id])).detach().numpy()[0]\n",
    "    return token_embedding[:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caabdda1-928f-4f60-8197-65dad7280635",
   "metadata": {},
   "source": [
    "Iterate through all the _step models and add up their embeddings for the magic tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "33f48757-e04f-462f-a0e7-5b0e00ba7ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders ending with '_step': ['bsb_model_step', 'greek_model_step', 'target_model_step']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(trained_model_folder) and os.path.isdir(trained_model_folder):\n",
    "    # Get a list of all subfolders in language_pair_staging_folder\n",
    "    subfolders = [folder for folder in os.listdir(trained_model_folder) if os.path.isdir(os.path.join(trained_model_folder, folder))]\n",
    "\n",
    "    # Filter subfolders that end with \"_model\"\n",
    "    model_folders = [folder for folder in subfolders if folder.endswith( \"_step\" ) ]\n",
    "\n",
    "    # Print or use the list of model folders\n",
    "    print(\"Folders ending with '_step':\", model_folders)\n",
    "else:\n",
    "    print(f\"The folder '{trained_model_folder}' does not exist or is not a directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1c9877e1-617b-447e-b05b-560700258618",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f63c613d6a4a4d9ab23e565e1df7df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid bâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grabbing embeddings from ./out/bsb_model_step\n",
      "Grabbing embeddings from ./out/bsb_model_step\n"
     ]
    }
   ],
   "source": [
    "embeddings_sum = {}\n",
    "embeddings_count = {}\n",
    "\n",
    "out = widgets.Output(layout={'border': '1px solid black'})\n",
    "display(out)\n",
    "\n",
    "for model_name in model_folders:\n",
    "    print( f\"Grabbing embeddings from {model_name}\" )\n",
    "    model_path = os.path.join( trained_model_folder, model_name )\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    input_embeddings = load_model(model_path).get_input_embeddings()\n",
    "    target_model = model_name.replace( \"_model\", \"\" ).replace( \"_step\", \"\" )\n",
    "    train_file_path = os.path.join( language_pair_staging_folder, f\"train_{target_model}.txt\" )\n",
    "    with open( train_file_path, \"rt\" ) as fin:\n",
    "        for line in fin:\n",
    "            line_tokenized = tokenizer.encode(line)\n",
    "            #print( f\"We got this for the line tokenized {line_tokenized}\" )\n",
    "            #now decode the token and see if it is one of our tokens if it starts with [\n",
    "            for token_id in line_tokenized:\n",
    "                token = tokenizer.decode([token_id])\n",
    "                is_magic_token = token.startswith( \"[\" ) and token.endswith( \"]\" ) and token[-3] == \"_\"\n",
    "                if( is_magic_token ):\n",
    "                    if( random.random() > .97 ):\n",
    "                        out.clear_output()\n",
    "                        with out:\n",
    "                            print( f\"Getting embedding for {token}\" )\n",
    "                    #print( f\"I see magic token {token}\" )\n",
    "                    #pull out the embedding for this specific token.\n",
    "                    embedding = input_embeddings(torch.LongTensor([token_id])).detach().numpy()\n",
    "                    #stash away the value so we can average it out.\n",
    "                    if token not in embeddings_count:\n",
    "                        embeddings_count[token] = 1\n",
    "                        embeddings_sum[token] = embedding\n",
    "                    else:\n",
    "                        embeddings_count[token] += 1\n",
    "                        embeddings_sum[token] += embedding\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8882332d-1266-4101-885d-99edbd546b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the actual average of the embeddings.\n",
    "embeddings_average = {}\n",
    "for token,embedding_sum in embeddings_sum.items():\n",
    "    embeddings_average[token] = embedding_sum/embeddings_count[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e5fa8049-9176-4efa-a7fe-fb4a9dd32488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macula_model: [ 0.01821761 -0.03325709 -0.01717107 -0.01796533]\n",
      "bsb_model_step: [ 0.01380756 -0.06270172  0.05579207 -0.01744836]\n",
      "greek_model: [ 0.01821761 -0.03325709 -0.01717107 -0.01796533]\n",
      "greek_model_step: [ 0.03959837 -0.01270676  0.065974   -0.05270576]\n",
      "target_model_step: [-0.04399784 -0.05841777  0.05558705  0.03709033]\n",
      "bsb_model: [ 0.01821761 -0.03325709 -0.01717107 -0.01796533]\n",
      "target_model: [ 0.01821761 -0.03325709 -0.01717107 -0.01796533]\n",
      "hebrew_model: [ 0.01821761 -0.03325709 -0.01717107 -0.01796533]\n"
     ]
    }
   ],
   "source": [
    "#print out a sample of the embeddings before updating them so we can see if they changed and how.\n",
    "subfolders = [folder for folder in os.listdir(trained_model_folder) if os.path.isdir(os.path.join(trained_model_folder, folder))]\n",
    "all_model_folders = [folder for folder in subfolders if folder.endswith( \"_model\" ) or folder.endswith( \"_step\" ) ]\n",
    "for model_name in all_model_folders:\n",
    "    print( f\"{model_name}: {sample_token_numbers( model_name )}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3773d347-3841-4016-afea-fcbd03f14afb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bsb_model_step', 'greek_model_step', 'target_model_step']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "2ee985fd-596e-4cf5-ba05-4590495f8d47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling in ./out/bsb_model_step\n",
      "Updating ./out/bsb_model\n",
      "Pulling in ./out/greek_model_step\n",
      "Updating ./out/greek_model\n",
      "Pulling in ./out/target_model_step\n",
      "Updating ./out/target_model\n"
     ]
    }
   ],
   "source": [
    "#now run back through all the models and inject the average embeddings.\n",
    "for model_name in model_folders:\n",
    "    #load the tokenizer and model\n",
    "    model_path = os.path.join( trained_model_folder, model_name )\n",
    "    print( f\"Pulling in {model_path}\" )\n",
    "    tokenizer = load_tokenizer(model_path)\n",
    "    model = load_model(model_path)\n",
    "    input_embeddings = model.get_input_embeddings()\n",
    "    \n",
    "    reconstructed_embedding_list = []\n",
    "    #run through all the tokens\n",
    "    for token_id in range(len(tokenizer)):\n",
    "        token = tokenizer.decode([token_id])\n",
    "        #and grab them from the average if it is in there.\n",
    "        if token in embeddings_average:\n",
    "            reconstructed_embedding_list.append( embeddings_average[token][0] )\n",
    "        else:\n",
    "            reconstructed_embedding_list.append( input_embeddings(torch.LongTensor([token_id])).detach().numpy()[0] )\n",
    "            \n",
    "    #construct it into some weights.\n",
    "    new_weights = torch.FloatTensor(reconstructed_embedding_list)\n",
    "    updated_embedding = nn.Embedding.from_pretrained(new_weights)\n",
    "    #put it back into the model\n",
    "    model.set_input_embeddings( updated_embedding )\n",
    "    model.tie_weights()\n",
    "\n",
    "    #save the model back to the non _step locations so the loop can begin again.    \n",
    "    output_dir = model_path.replace( \"_step\", \"\" )\n",
    "    print( f\"Updating {output_dir}\" )\n",
    "    model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "de29f982-b143-4d10-a7ca-e78e325c5860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bsb_model_step: [ 0.01380756 -0.06270172  0.05579207 -0.01744836]\n",
      "greek_model: [ 0.01380756 -0.06270172  0.05579207 -0.01744836]\n",
      "greek_model_step: [ 0.03959837 -0.01270676  0.065974   -0.05270576]\n",
      "target_model_step: [-0.04399784 -0.05841777  0.05558705  0.03709033]\n",
      "bsb_model: [ 0.01380756 -0.06270172  0.05579207 -0.01744836]\n",
      "target_model: [ 0.01380756 -0.06270172  0.05579207 -0.01744836]\n",
      "hebrew_model: [ 0.01821761 -0.03325709 -0.01717107 -0.01796533]\n"
     ]
    }
   ],
   "source": [
    "#now run through all the models and sample their numbers again to see how they changed.\n",
    "subfolders = [folder for folder in os.listdir(trained_model_folder) if os.path.isdir(os.path.join(trained_model_folder, folder))]\n",
    "all_model_folders = [folder for folder in subfolders if folder.endswith( \"_model\" ) or folder.endswith( \"_step\" ) ]\n",
    "for model_name in all_model_folders:\n",
    "    print( f\"{model_name}: {sample_token_numbers( model_name )}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
