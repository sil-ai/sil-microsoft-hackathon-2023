{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Glossary\n",
    "\n",
    "In theory GPT4 can take a small amount of text and find the obvious words and how they are used.  Over time if given more sections it should learn more and be able to refine the glossary.\n",
    "\n",
    " - [ ] Given a portion of Scripture generate a glossary\n",
    " - [ ] Expand the glossary by adding the old glossary and more text and ask GPT to output new words and edits to words (to reduce tokens)\n",
    " - [ ] Given a completed draft glossary loop word by word and find references of the word being used and ask GPT to improve the glossary\n",
    " - [ ] Save the glossary to a database and make it editable, indexes on potential words\n",
    " - [ ] On requesting translation add the potential words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some defaults\n",
    "TRAINING_SOURCE = ['MRK']\n",
    "TEST_SOURCE = ['MAT']\n",
    "GPT_VERSION = 'gpt-4-32k'\n",
    "TOKENS_RESERVED_FOR_NEW_GRAMMER_PER_WORD = 2000\n",
    "VERSION = 'birrig'  # target version\n",
    "SOURCES = ['source_content']\n",
    "SAMPLES_PER_WORD = 50\n",
    "MAX_WORDS_PER_BATCH = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and fixes to pathing\n",
    "%reload_ext autoreload\n",
    "import sys\n",
    "sys.path.append('../lib')\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import openai, time, os\n",
    "from openai.error import RateLimitError, OpenAIError\n",
    "from config import get_config\n",
    "from collections import defaultdict\n",
    "from cipher import substitution_cipher\n",
    "\n",
    "openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = get_config('openai')['api_type']\n",
    "openai.api_base = os.environ[\"OPENAI_API_BASE\"] = get_config('openai')['api_base']\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"] = get_config('openai')['api_key']\n",
    "openai.api_version = os.environ[\"OPENAI_API_VERSION\"] = get_config('openai')['api_version']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the langauge\n",
    "\n",
    "Based on [https://github.com/ChrisPriebe/BibleTranslation/blob/exp/test-basic/get_bible.ipynb] encode the English version BBE using a letter substitution cipher.  This ensure we have 100% new words and simulates a new language.\n",
    "\n",
    "PRO\n",
    " - Starts with an empty language\n",
    " - By using BBE as an input and training it on translating a non-BBE version it prevents word to word translation.  It needs to capture meanings\n",
    "\n",
    "CONS\n",
    " - Does not reflect the linguisitical nuances like stemming, changes to word order, etc in other languages.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vref</th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>verse</th>\n",
       "      <th>eng-web</th>\n",
       "      <th>eng-asv</th>\n",
       "      <th>eng-kjv2006</th>\n",
       "      <th>engBBE</th>\n",
       "      <th>hin2017</th>\n",
       "      <th>arbnav</th>\n",
       "      <th>latVUC</th>\n",
       "      <th>amo</th>\n",
       "      <th>source_content</th>\n",
       "      <th>birrig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GEN 1:1</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the beginning, God created the heavens and ...</td>\n",
       "      <td>In the beginning God created the heavens and t...</td>\n",
       "      <td>In the beginning God created the heaven and th...</td>\n",
       "      <td>At the first God made the heaven and the earth.</td>\n",
       "      <td>आदि में परमेश्‍वर ने आकाश और पृथ्वी की सृष्टि ...</td>\n",
       "      <td>فِي الْبَدْءِ خَلَقَ اللهُ السَّمَاوَاتِ وَالأ...</td>\n",
       "      <td>In principio creavit Deus cælum et terram.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>בְּרֵאשִׁ֖ית  בָּרָ֣א  אֱלֹהִ֑ים  אֵ֥ת  הַשָּׁ...</td>\n",
       "      <td>El lxi sovzl Guw newi lxi xiemir erw lxi ievlx.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEN 1:2</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The earth was formless and empty. Darkness was...</td>\n",
       "      <td>And the earth was waste and void; and darkness...</td>\n",
       "      <td>And the earth was without form, and void; and ...</td>\n",
       "      <td>And the earth was waste and without form; and ...</td>\n",
       "      <td>पृथ्वी बेडौल और सुनसान पड़ी थी, और गहरे जल के ...</td>\n",
       "      <td>وَإِذْ كَانَتِ الأَرْضُ مُشَوَّشَةً وَمُقْفِرَ...</td>\n",
       "      <td>Terra autem erat inanis et vacua, et tenebræ e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וְהָאָ֗רֶץ  הָיְתָ֥ה  תֹ֨הוּ֙  וָבֹ֔הוּ  וְחֹ֖...</td>\n",
       "      <td>Erw lxi ievlx hez hezli erw holxual suvn; erw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEN 1:3</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>God said, “Let there be light,” and there was ...</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>तब परमेश्‍वर ने कहा, “उजियाला हो*,” तो उजियाला...</td>\n",
       "      <td>أَمَرَ اللهُ: «لِيَكُنْ نُورٌ». فَصَارَ نُورٌ،</td>\n",
       "      <td>Dixitque Deus: Fiat lux. Et facta est lux.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וַיֹּ֥אמֶר  אֱלֹהִ֖ים  יְהִ֣י  א֑וֹר  וַֽיְהִי...</td>\n",
       "      <td>Erw Guw zeow, Pil lxivi fi pogxl: erw lxivi he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GEN 1:4</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>God saw the light, and saw that it was good. G...</td>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>And God, looking on the light, saw that it was...</td>\n",
       "      <td>और परमेश्‍वर ने उजियाले को देखा कि अच्छा है*; ...</td>\n",
       "      <td>وَرَأَى اللهُ النُّورَ فَاسْتَحْسَنَهُ وَفَصَل...</td>\n",
       "      <td>Et vidit Deus lucem quod esset bona: et divisi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וַיַּ֧רְא  אֱלֹהִ֛ים  אֶת־ הָא֖וֹר  כִּי־ ט֑וֹ...</td>\n",
       "      <td>Erw Guw, puucorg ur lxi pogxl, zeh lxel ol hez...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GEN 1:5</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>God called the light “day”, and the darkness h...</td>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>Naming the light, Day, and the dark, Night. An...</td>\n",
       "      <td>और परमेश्‍वर ने उजियाले को दिन और अंधियारे को ...</td>\n",
       "      <td>وَسَمَّى اللهُ النُّورَ نَهَاراً، أَمَّا الظَّ...</td>\n",
       "      <td>Appellavitque lucem Diem, et tenebras Noctem: ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וַיִּקְרָ֨א  אֱלֹהִ֤ים׀ לָאוֹר֙  י֔וֹם  וְלַחֹ...</td>\n",
       "      <td>Renorg lxi pogxl, Wej, erw lxi wevc, Rogxl. Er...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      vref book  chapter  verse  \\\n",
       "0  GEN 1:1  GEN        1      1   \n",
       "1  GEN 1:2  GEN        1      2   \n",
       "2  GEN 1:3  GEN        1      3   \n",
       "3  GEN 1:4  GEN        1      4   \n",
       "4  GEN 1:5  GEN        1      5   \n",
       "\n",
       "                                             eng-web  \\\n",
       "0  In the beginning, God created the heavens and ...   \n",
       "1  The earth was formless and empty. Darkness was...   \n",
       "2  God said, “Let there be light,” and there was ...   \n",
       "3  God saw the light, and saw that it was good. G...   \n",
       "4  God called the light “day”, and the darkness h...   \n",
       "\n",
       "                                             eng-asv  \\\n",
       "0  In the beginning God created the heavens and t...   \n",
       "1  And the earth was waste and void; and darkness...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                         eng-kjv2006  \\\n",
       "0  In the beginning God created the heaven and th...   \n",
       "1  And the earth was without form, and void; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                              engBBE  \\\n",
       "0    At the first God made the heaven and the earth.   \n",
       "1  And the earth was waste and without form; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God, looking on the light, saw that it was...   \n",
       "4  Naming the light, Day, and the dark, Night. An...   \n",
       "\n",
       "                                             hin2017  \\\n",
       "0  आदि में परमेश्‍वर ने आकाश और पृथ्वी की सृष्टि ...   \n",
       "1  पृथ्वी बेडौल और सुनसान पड़ी थी, और गहरे जल के ...   \n",
       "2  तब परमेश्‍वर ने कहा, “उजियाला हो*,” तो उजियाला...   \n",
       "3  और परमेश्‍वर ने उजियाले को देखा कि अच्छा है*; ...   \n",
       "4  और परमेश्‍वर ने उजियाले को दिन और अंधियारे को ...   \n",
       "\n",
       "                                              arbnav  \\\n",
       "0  فِي الْبَدْءِ خَلَقَ اللهُ السَّمَاوَاتِ وَالأ...   \n",
       "1  وَإِذْ كَانَتِ الأَرْضُ مُشَوَّشَةً وَمُقْفِرَ...   \n",
       "2     أَمَرَ اللهُ: «لِيَكُنْ نُورٌ». فَصَارَ نُورٌ،   \n",
       "3  وَرَأَى اللهُ النُّورَ فَاسْتَحْسَنَهُ وَفَصَل...   \n",
       "4  وَسَمَّى اللهُ النُّورَ نَهَاراً، أَمَّا الظَّ...   \n",
       "\n",
       "                                              latVUC  amo  \\\n",
       "0         In principio creavit Deus cælum et terram.  NaN   \n",
       "1  Terra autem erat inanis et vacua, et tenebræ e...  NaN   \n",
       "2         Dixitque Deus: Fiat lux. Et facta est lux.  NaN   \n",
       "3  Et vidit Deus lucem quod esset bona: et divisi...  NaN   \n",
       "4  Appellavitque lucem Diem, et tenebras Noctem: ...  NaN   \n",
       "\n",
       "                                      source_content  \\\n",
       "0  בְּרֵאשִׁ֖ית  בָּרָ֣א  אֱלֹהִ֑ים  אֵ֥ת  הַשָּׁ...   \n",
       "1  וְהָאָ֗רֶץ  הָיְתָ֥ה  תֹ֨הוּ֙  וָבֹ֔הוּ  וְחֹ֖...   \n",
       "2  וַיֹּ֥אמֶר  אֱלֹהִ֖ים  יְהִ֣י  א֑וֹר  וַֽיְהִי...   \n",
       "3  וַיַּ֧רְא  אֱלֹהִ֛ים  אֶת־ הָא֖וֹר  כִּי־ ט֑וֹ...   \n",
       "4  וַיִּקְרָ֨א  אֱלֹהִ֤ים׀ לָאוֹר֙  י֔וֹם  וְלַחֹ...   \n",
       "\n",
       "                                              birrig  \n",
       "0    El lxi sovzl Guw newi lxi xiemir erw lxi ievlx.  \n",
       "1  Erw lxi ievlx hez hezli erw holxual suvn; erw ...  \n",
       "2  Erw Guw zeow, Pil lxivi fi pogxl: erw lxivi he...  \n",
       "3  Erw Guw, puucorg ur lxi pogxl, zeh lxel ol hez...  \n",
       "4  Renorg lxi pogxl, Wej, erw lxi wevc, Rogxl. Er...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Read the data/berrig.csv file into dataframe\n",
    "df = pd.read_csv('../data/birrig.csv')\n",
    "# rename df[0] to df['vref']\n",
    "df.rename(columns={df.columns[0]: 'vref'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all of eng-web and count word by word and tell me which is the most common word\n",
    "# and how many times it occurs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE: 678 verses\n",
      "TEST SIZE: 1071 verses\n"
     ]
    }
   ],
   "source": [
    "training_df = df[df['book'].isin(TRAINING_SOURCE)]\n",
    "test_df = df[df['book'].isin(TEST_SOURCE)]\n",
    "print(f\"TRAIN SIZE: {len(training_df)} verses\")\n",
    "print(f\"TEST SIZE: {len(test_df)} verses\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>cipher_train</th>\n",
       "      <th>frequency_train</th>\n",
       "      <th>cipher_test</th>\n",
       "      <th>frequency_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>erw</td>\n",
       "      <td>and</td>\n",
       "      <td>1197.0</td>\n",
       "      <td>and</td>\n",
       "      <td>1488.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lxi</td>\n",
       "      <td>the</td>\n",
       "      <td>942.0</td>\n",
       "      <td>the</td>\n",
       "      <td>1601.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lu</td>\n",
       "      <td>to</td>\n",
       "      <td>639.0</td>\n",
       "      <td>to</td>\n",
       "      <td>977.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>us</td>\n",
       "      <td>of</td>\n",
       "      <td>476.0</td>\n",
       "      <td>of</td>\n",
       "      <td>861.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xi</td>\n",
       "      <td>he</td>\n",
       "      <td>397.0</td>\n",
       "      <td>he</td>\n",
       "      <td>414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>zruh:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>snow:</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>uvwiviw:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ordered:</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>kavvirl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>current</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>qvizirl</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>present</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>huvzxoq:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>worship:</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1649 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word cipher_train  frequency_train cipher_test  frequency_test\n",
       "0          erw          and           1197.0         and          1488.0\n",
       "1          lxi          the            942.0         the          1601.0\n",
       "2           lu           to            639.0          to           977.0\n",
       "3           us           of            476.0          of           861.0\n",
       "4           xi           he            397.0          he           414.0\n",
       "...        ...          ...              ...         ...             ...\n",
       "1644     zruh:          NaN              NaN       snow:             1.0\n",
       "1645  uvwiviw:          NaN              NaN    ordered:             1.0\n",
       "1646   kavvirl          NaN              NaN     current             1.0\n",
       "1647   qvizirl          NaN              NaN     present             1.0\n",
       "1648  huvzxoq:          NaN              NaN    worship:             1.0\n",
       "\n",
       "[1649 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word analysis\n",
    "# Determine the most common words in the train data\n",
    "\n",
    "def get_words(df, version='birrig', min_frequency=0, min_length=0, cipher_decode=True):\n",
    "    word_count = defaultdict(int)\n",
    "    for index, row in df.iterrows():\n",
    "        for word in row[version].split():\n",
    "            # lowercase and remove punctation\n",
    "            word = word.lower().strip('.,;!?')\n",
    "            if len(word) >= min_length:\n",
    "                word_count[word] += 1\n",
    "\n",
    "    # Sort the words by frequency\n",
    "    sorted_words = sorted(word_count.items(), key=lambda item: item[1], reverse=cipher_decode)\n",
    "    all_words = [(word[0], substitution_cipher(word[0], encode=False).strip(), word[1]) for word in sorted_words]\n",
    "    \n",
    "    # reduce words to those that appear more than min_frequency\n",
    "    return [word for word in all_words if word[2] > min_frequency]\n",
    "\n",
    "words = get_words(training_df)\n",
    "# convert words to dataframe\n",
    "words_df = pd.DataFrame(words, columns=['word', 'cipher', 'frequency'])\n",
    "test_words = get_words(test_df)\n",
    "# convert words to dataframe\n",
    "test_words_df = pd.DataFrame(test_words, columns=['word', 'cipher', 'frequency'])\n",
    "\n",
    "# merge the two dataframes\n",
    "words_df = words_df.merge(test_words_df, on='word', how='outer', suffixes=('_train', '_test'))\n",
    "words_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ONLY WORDS: 209\n",
      "TEST ONLY WORDS: 487\n",
      "OVERLAPPING WORDS: 953\n"
     ]
    }
   ],
   "source": [
    "# how many words are in training but not test\n",
    "print(f\"TRAINING ONLY WORDS: {len(words_df[words_df['frequency_test'].isna()])}\")\n",
    "print(f\"TEST ONLY WORDS: {len(words_df[words_df['frequency_train'].isna()])}\")\n",
    "print(f\"OVERLAPPING WORDS: {len(words_df[words_df['frequency_train'].notna() & words_df['frequency_test'].notna()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order the data to learn the most common first\n",
    "Instead of randomly learning words start with the most common (a, the, Jesus, etc) so they are known to be correct first, then work your way down the list.  This should help with the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT ANALYSIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "# Role\n",
    "You are an expert linguistic and polyglot.  You love to learn new languages.  You also have a doctorate of Theology and are fluent in the Biblical languages and the original word meanings.  \n",
    "\n",
    "# Task\n",
    "You will be given a new language and you are to create a glossary and dictionary for each word in the targeted words list.  \n",
    "\n",
    "Here are the steps you must return.  Show your work for each step.\n",
    "\n",
    "## Step 1\n",
    "The word we added is in all the target verses we provided therefore if you make a list of all the most common words that their concept appears is all the source languages you will get our word. \n",
    "\n",
    "Make that list now of what meanings appear in all words.  (If there are no words that appear in all assume this word has more than one meaning and find all those meanings)\n",
    "\n",
    "Create a dictionary of words and meanings. Adding it to the dictionary the user provides (if they provide one)\n",
    "\n",
    "## Step 2\n",
    "For each targeted word look at all the verses it appears in.  What is the most likely word that appears in all those verses and is not yet defined in our dictionary.  What is the second most likely word and third.\n",
    "If that word is already taken by another word in the dictionary then reanalyze that word as it might be an eariler error and add that word to our targetted words too.\n",
    "\n",
    "# Step 3\n",
    "Disagree with your answer in Step 2 and argue why it could be a different word\n",
    "\n",
    "# Step 4\n",
    "Create the glossary in the format provided below.\n",
    "\n",
    "Here are some guidelines\n",
    "\n",
    " - Use the vref to look up the original language, NIV, NKJV, Amplified, Swahali, Arabic, Chinese, German and French translations.  I am also providing the original text and the targeted text. \n",
    " - Think step by step.  Propose 2 to 3 words it could be.  Explain why by quoting sentence fragments from English, Greek, German, Arabic, Swahili, Hebrew.\n",
    "\n",
    "# Glossary format\n",
    "Return the results in properly formatted JSON inside a code block\n",
    "\n",
    "word: string\n",
    "\n",
    "strongs: (string[]) array of strongs concordance numbers in order of likely meaning (max 3)\n",
    "\n",
    "english: (string[]) array of english words in order of likely meaning (max 3)\n",
    "\n",
    "grammer: (char[])  array of grammer codes in order of likely meaning (part of speech, Tense, number, gender, stem (which word it is derived from), etc) Focus only on this targetted word and think step by step through the examples of how it was used.\n",
    "\n",
    "debug: (string: max 200 words) Show your work here, what words did you consider, why is this one picked, quote sentence fragments from Greek and English to show why.  What is an alternate word it could mean?  \n",
    "\n",
    "exceptions: (string): Note any edge cases, exceptions, unique language rules for this word.\n",
    "\n",
    "# Input format\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "input_format = \"\"\"\n",
    "## Targeted Words\n",
    "{targeted_words}\n",
    "\n",
    "## Current glossary (if any)\n",
    "```\n",
    "word\\tdefinition\n",
    "```\n",
    "{definitions}\n",
    "\n",
    "## Training Data\n",
    "```\n",
    "| Bible Verse Reference | Target Language to Learn |\n",
    "{verses}\n",
    "```\n",
    "\"\"\"\n",
    "system_message += input_format\n",
    "\n",
    "def format_verses(row, version='source_content'):\n",
    "    source = row[version].replace(\"|\",\":\")\n",
    "    target = row[VERSION].replace(\"|\",\":\")\n",
    "    return f\"\"\"\n",
    "| {row['vref']} | {source} | {target} |\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt(system_message, content, max_tokens, debug=False):\n",
    "    messages = [\n",
    "                {\"role\":\"system\",\"content\": system_message},\n",
    "                {\"role\":\"user\",\"content\": content}\n",
    "            ]\n",
    "    start_time = time.time()\n",
    "    #print(\"Calling GPT\", messages)\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=GPT_VERSION,\n",
    "            messages=messages,\n",
    "            temperature=0.01,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        result = response.get('choices',[{}])[0].get('message',{'content':''}).get('content','')  \n",
    "        #print(\"GPT Response\", time.time()-start_time, result)\n",
    "        return result\n",
    "    \n",
    "    except RateLimitError as e:\n",
    "        print(f\"Rate Limit Error: {e}\")\n",
    "        time.sleep(30)\n",
    "        \n",
    "    except OpenAIError as e:\n",
    "        print(f\"OpenAI Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ol': {'word': 'ol',\n",
       "  'strongs': ['G2532', 'G1161', 'G1063'],\n",
       "  'english': ['and', 'but', 'for'],\n",
       "  'grammer': ['CC', 'CC', 'CC'],\n",
       "  'debug': \"Considered words: and, but, for. Picked 'ol' because it appears in all verses and matches the meanings of 'and', 'but', 'for' in Greek and English. Alternate word: then, so.\",\n",
       "  'exceptions': \"Might also mean 'then' or 'so' in some contexts.\"}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re, json\n",
    "###\n",
    "# Extract glossary words form a GPT response\n",
    "\n",
    "def extract_glossary(response, glossary={}):\n",
    "    if not response:\n",
    "        return glossary\n",
    "    \n",
    "    # use a regex to find all the blocks inside a markdown block quote\n",
    "    matches = re.findall(r'```(.+?)```', response, re.DOTALL)\n",
    "    new_words = {}\n",
    "    for match in matches:\n",
    "        # if it is json then use it, otherwise ignore it\n",
    "        try:\n",
    "            result = json.loads(match)\n",
    "            new_words[result['word']] = result\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "\n",
    "\n",
    "    return new_words\n",
    "\n",
    "    # for line in response.split('\\n'):\n",
    "    #     if line.startswith('|'):\n",
    "    #         parts = line.split('|')\n",
    "    #         if len(parts) >= 5:\n",
    "    #             word = parts[1].strip()\n",
    "    #             decoded = substitution_cipher(word, encode=False).strip()\n",
    "    #             strongs = parts[2].strip()\n",
    "    #             english = parts[3].strip()\n",
    "    #             grammer = parts[4].strip()\n",
    "    #             glossary[word] = {'decoded': decoded, 'strongs': strongs, 'english': english, 'grammer': grammer, 'row': line}\n",
    "    # return glossary\n",
    "\n",
    "example = \"## Step 1\\nThe word \\\"ol\\\" appears in all the target verses. The most common meanings that appear in all source languages are:\\n\\n1. And\\n2. But\\n3. For\\n\\nCreating a dictionary of words and meanings:\\n\\n```\\nword    definition\\nol      and\\nol      but\\nol      for\\n```\\n\\n## Step 2\\nFor each targeted word, the most likely word that appears in all those verses and is not yet defined in our dictionary is \\\"ol\\\". The second most likely word is \\\"erw\\\", and the third most likely word is \\\"xi\\\".\\n\\n## Step 3\\nIt could be argued that \\\"ol\\\" might also mean \\\"then\\\" or \\\"so\\\" in some contexts, as these words also appear frequently in the verses provided.\\n\\n## Step 4\\nCreating the glossary:\\n\\n```\\n{\\n  \\\"word\\\": \\\"ol\\\",\\n  \\\"strongs\\\": [\\\"G2532\\\", \\\"G1161\\\", \\\"G1063\\\"],\\n  \\\"english\\\": [\\\"and\\\", \\\"but\\\", \\\"for\\\"],\\n  \\\"grammer\\\": [\\\"CC\\\", \\\"CC\\\", \\\"CC\\\"],\\n  \\\"debug\\\": \\\"Considered words: and, but, for. Picked 'ol' because it appears in all verses and matches the meanings of 'and', 'but', 'for' in Greek and English. Alternate word: then, so.\\\",\\n  \\\"exceptions\\\": \\\"Might also mean 'then' or 'so' in some contexts.\\\"\\n}\\n```\"\n",
    "extract_glossary(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test it\n",
    "glossary = extract_glossary(\"\"\"| word | strongs | english | grammer |\n",
    "|------|---------|---------|---------|\n",
    "| erw  | G2532   | and     | CC      |\n",
    "| lxi  | G3588   | the     | T-NSF   |\n",
    "| xon  | G846    | him     | P-DSM   |\n",
    "| lu   | G4314   | to      | P       |\n",
    "| xi   | G3588   | the     | T-DSM   |\n",
    "| zeow | G3004   | say     | V-PAI-3S|\n",
    "| jua  | G4771   | you     | P-2DP   |\n",
    "| ol   | G3756   | not     | D       |\n",
    "| zii  | G5100   | some    | PX-NSM  |\n",
    "| lxin | G846    | them    | P-DPM   |\n",
    "| hivi | G1096   | be      | V-PNI-3P|\n",
    "| us   | G1537   | from    | P       |\n",
    "| gu   | G1519   | into    | P       |\n",
    "| or   | G1722   | in      | P       |\n",
    "| fal  | G1161   | but     | CC      |\n",
    "| zu   | G2443   | so that | CS      |\n",
    "| lopp | G2193   | until   | CS      |\n",
    "| lxel | G3704   | when    | CS      |\n",
    "| lopp | G2193   | until   | CS      |\n",
    "| lxi  | G3588   | the     | T-NSM   |\n",
    "| hopp | G3361   | not     | D       |\n",
    "| rul  | G1410   | able    | V-PNI-3S|\n",
    "| leci | G2983   | take    | V-2AAS-3S|\n",
    "| ni   | G1722   | in      | P       |\n",
    "| az   | G1473   | us      | P-1GP   |\n",
    "| xemi | G5100   | any     | PX-ASN  |\n",
    "| ru   | G1722   | in      | P       |\n",
    "| nurij| G3778   | this    | PD-DSF  |\n",
    "| wuri | G3704   | so that | CS      |\n",
    "| zu   | G2443   | so that | CS      |\n",
    "| lxel | G3704   | when    | CS      |\n",
    "| lxi  | G3588   | the     | T-NSF   |\n",
    "| hopp | G3361   | not     | D       |\n",
    "| rul  | G1410   | able    | V-PNI-3S|\n",
    "| leci | G2983   | take    | V-2AAS-3S|\n",
    "| ni   | G1722   | in      | P       |\n",
    "| az   | G1473   | us      | P-1GP   |\n",
    "| xemi | G5100   | any     | PX-ASN  |\n",
    "| ru   | G1722   | in      | P       |\n",
    "| nurij| G3778   | this    | PD-DSF  |\"\"\",{})\n",
    "\n",
    "# convert glossary into a dataframe\n",
    "pd.DataFrame(glossary).transpose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis\n",
    "\n",
    "The above was from asking for \n",
    "\n",
    "## Targetted Words\n",
    " - erw\n",
    " - lxi\n",
    " - lu\n",
    " - us\n",
    " - xi\n",
    " - xon\n",
    " - lxij\n",
    " - e\n",
    " - zeow\n",
    " - or\n",
    " - jua\n",
    "\n",
    "The other words it thought it has confidence on but I think we should leave them off as it doesn't have enough context yet.\n",
    "\n",
    "It got xi wrong so I said\n",
    "\n",
    "The word xi is not \"the\"  Please think step by step and let me know what it should be, show your work\n",
    "\n",
    "It replied\n",
    "\n",
    "```\n",
    "I apologize for the confusion. I will reanalyze the word \"xi\" and provide the correct translation with my work.\n",
    "\n",
    "In the provided verses, \"xi\" appears to be functioning as a pronoun rather than the article \"the.\" Let's take a closer look at one of the verses:\n",
    "\n",
    "MRK 5:31: καὶ ἔλεγον αὐτῷ οἱ μαθηταὶ αὐτοῦ Βλέπεις τὸν ὄχλον συνθλίβοντά σε, καὶ λέγεις Τίς μου ἥψατο;\n",
    "And his disciples said to him, \"You see the crowd pressing around you, and yet you say, 'Who touched me?'\"\n",
    "\n",
    "In this verse, \"xi\" is used as a pronoun referring to Jesus (\"him\"):\n",
    "\n",
    "Erw xoz wozkoqpiz zeow lu xon, Jua zii lxi qiuqpi vuarw jua ur imivj zowi, erw xi zeow, Hxj evi jua zlopp lvuafporg lxi Nezliv?\n",
    "\n",
    "I will now update the glossary entry for \"xi\":\n",
    "word\tstrongs\tenglish\tgrammer\n",
    "xi\tG846\thim, his\tD\n",
    "```\n",
    "\n",
    "I like the him, his\n",
    "We may need to do a step by step for each targetted word like this and then a summary table at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD:  erw CORRECT:  and ['and', 'but', 'for'] Considered words: and, but, for. Picked 'and' because it appears in all verses and is a common conjunction. Alternate words: then, so.\n",
      "Rate Limit Error: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 45 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\n",
      "Rate Limit Error: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 15 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\n",
      "Rate Limit Error: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 17 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\n",
      "Rate Limit Error: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded token rate limit of your current OpenAI S0 pricing tier. Please retry after 32 seconds. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit.\n"
     ]
    }
   ],
   "source": [
    "# every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "tokenizer = tiktoken.encoding_for_model('gpt-4')\n",
    "tokens_left = 32000\n",
    "\n",
    "frozen_tokens_left = tokens_left\n",
    "glossary = {}\n",
    "\n",
    "# Reest these after each GPT call\n",
    "# We only have so many tokens to work with so we need to break up the data\n",
    "verses = \"\" # The verses we are learning from\n",
    "glossary_words = \"\"  # The words we already know\n",
    "glossary_current = {} # Quick lookup table of words we already know to prevent duplication\n",
    "targeted_words = {}\n",
    "\n",
    "class GlossaryCreator:\n",
    "    def __init__(self, training_df, words, target_language='birrig'):\n",
    "        self.df = training_df.copy()\n",
    "        self.df['normalized'] = self.df[target_language].apply(self.normalize_text)\n",
    "        # drop all rows that have no value in normalized\n",
    "        self.df = self.df[self.df['normalized'].notna()]\n",
    "        self.words = words\n",
    "        self.index = 0\n",
    "        self.target_language = target_language\n",
    "        self.glossary = {}\n",
    "        self.definitions = {}\n",
    "        self.load_dictionary()\n",
    "        self._reset_for_next_call()\n",
    "\n",
    "    def load_dictionary(self):\n",
    "        dictionary = pd.read_csv('dictionary.csv')\n",
    "        # export pandas dataframe to dictionary\n",
    "        for index, row in dictionary.iterrows():\n",
    "            self.definitions[self.normalize_text(row['target'])] = row['source']\n",
    "\n",
    "    def run(self):\n",
    "        for (word, decoded, count) in self.words:\n",
    "            self._next_word(word)\n",
    "        # Flush our final data to GPT as the last batch won't get called above\n",
    "        self.call_gpt()\n",
    "        return self.glossary\n",
    "\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        # Remove punc and lowercase all words\n",
    "        # TODO: for more languages you can use unicode base tools to look at the type of char it is\n",
    "        # and if type of punc then skip it.\n",
    "        return \" \".join([ word.lower().strip('.,;!?[]{}()\\\\') for word in text.split()])\n",
    "\n",
    "    def format_verse(self, verse):\n",
    "        return f\"{verse['vref']}\\t{verse['source_content']}\\t{verse[self.target_language]}\" or ''\n",
    "        #return f\"{verse['vref']}\\t{verse[self.target_language]}\" or ''\n",
    "\n",
    "    def format_word_definitions(self, word_definitions):\n",
    "        return \"\\n\".join([f\"{word}\\t{definition}\" for (word, definition) in word_definitions.items() if definition])\n",
    "\n",
    "    def get_used_word_definitions(self, verses_df):\n",
    "        # Get all the words used in the verses\n",
    "        words_used = {}\n",
    "        for index, row in verses_df.iterrows():\n",
    "            for word in row['normalized'].split():\n",
    "                if word not in self.glossary_words:\n",
    "                    words_used[word] = self.definitions.get(word, None)\n",
    "        return words_used\n",
    "\n",
    "    def _reset_for_next_call(self):\n",
    "        self._reset_tokens()\n",
    "        self.verses = \"\"\n",
    "        self.glossary_words = {}\n",
    "        self.targetted_words = []\n",
    "        self.word_in_batch = 0\n",
    "\n",
    "    def _reset_tokens(self):\n",
    "        self.tokens_available = 32000\n",
    "        self.tokens_available -= len(tokenizer.encode(system_message))-4\n",
    "        self.tokens_available -= len(tokenizer.encode(input_format))-4        \n",
    "\n",
    "    def _next_word(self, word):\n",
    "        # find all verses that contain the word\n",
    "        # shuffle the verses so we don't keep getting the same ones\n",
    "        df = self.df.sample(frac=1)\n",
    "        # Find x verses that have this word in it\n",
    "        normalized = self.normalize_text(word)\n",
    "        verses = df[df['normalized'].str.contains(normalized)].head(SAMPLES_PER_WORD)\n",
    "        verses_text = \"\\n\".join(verses.apply(self.format_verse, axis=1))\n",
    "        new_word_definitions = self.get_used_word_definitions(verses)\n",
    "        new_word_definition_text = self.format_word_definitions(new_word_definitions)\n",
    "\n",
    "        # See if we have enough tokens to add this word\n",
    "        tokens_used = (len(tokenizer.encode(verses_text))+1)*2 # +1 for the newline, # *2 becuase it seems toktoken is not good at non normal languages\n",
    "        tokens_used += len(tokenizer.encode(word+\"\\n\"))\n",
    "        tokens_used += len(tokenizer.encode(new_word_definition_text))\n",
    "        tokens_used += TOKENS_RESERVED_FOR_NEW_GRAMMER_PER_WORD\n",
    "\n",
    "        if self.tokens_available - tokens_used > 0:\n",
    "            # Push these changes to our data\n",
    "            self.verses += \"\\n\" + verses_text\n",
    "            self.targetted_words.append(word)\n",
    "            self.glossary_words.update(new_word_definitions)\n",
    "            self.tokens_available -= tokens_used\n",
    "            self.word_in_batch += 1\n",
    "            if self.word_in_batch >= MAX_WORDS_PER_BATCH:\n",
    "                self.call_gpt()\n",
    "                self._reset_for_next_call()\n",
    "\n",
    "        else:\n",
    "            self.call_gpt()\n",
    "            self._reset_for_next_call()\n",
    "            self.verses = verses_text\n",
    "            self.targetted_words = [word]\n",
    "            # Rebuild glossary words as the glossary changed and we need all the words we are using\n",
    "            self.glossary_words = self.get_used_word_definitions(verses)\n",
    "\n",
    "    def call_gpt(self):\n",
    "        definitions = self.format_word_definitions(self.glossary_words)\n",
    "        content = input_format.format(definitions=definitions, verses=self.verses, targeted_words=\"\\n\".join(self.targetted_words))\n",
    "        #print(content)\n",
    "        for attempt in range(3):\n",
    "            # -100 is just a buffer as you will get an openai error if you calculated it wrongly\n",
    "            response = call_gpt(system_message, content, max_tokens=(self.tokens_available-100),debug=True)\n",
    "            if response:\n",
    "                break\n",
    "        \n",
    "        new_words = extract_glossary(response, self.glossary)\n",
    "        for word, metadata in new_words.items():\n",
    "            self.definitions[word] = json.dumps(metadata['english'])\n",
    "            print(\"WORD: \", word, \"CORRECT: \", substitution_cipher(word, encode=False), metadata['english'], metadata['debug'])\n",
    "        self.glossary.update(new_words)\n",
    "        self.save_glossary()\n",
    "\n",
    "    def save_glossary(self):\n",
    "        # convert glossary into a dataframe\n",
    "        pd.DataFrame(self.glossary).transpose().to_csv('glossary.csv')\n",
    "        \n",
    "\n",
    "glossary = GlossaryCreator(training_df, words).run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and further work\n",
    "\n",
    "It appears to be struggling with some of the words, I had thoughts addnig more steps would fix it but it is choosing odd words like for \"us\" means \"of\" but chose son which is close enough.  \n",
    "\n",
    "For xi (he) it is determined it is and, then or so.  \n",
    "\n",
    "Likewise lu (to) it thinks is say (I guess said to them)\n",
    "\n",
    "## Alternate approach\n",
    "Try a majority vote.  Have it guess on all words provided and return more than just the targetted words\n",
    "For targetted words give a strong weight say 10 points\n",
    "But for additional words only 1 vote for that word\n",
    "\n",
    "Go through the corpus a few times adding more words it might know\n",
    "\n",
    "## Let it find it's errors\n",
    "If given more time to run it may realize that xi cannot be and because another word has to be and\n",
    "So given an update in the prompt it may be able to fix itself\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternate Approach - Statical Analysis\n",
    "In theory if did a word search for all verses with word X in it then took those verses and did a word frequency analysis on the words in those verses it should be able to determine the most common words used with word X.  This would be a good starting point for the glossary.\n",
    "\n",
    "We can then form a list of all possible words and feed it into GPT to determine likely words\n",
    "\n",
    "Some considerations and cautions\n",
    " - Some languages like Greek have multiple words for words like \"the\" depending if it is male,female, neuter, singular, plural, etc.  Other words may not have the same so it may be a one to many relationship\n",
    " - Likewise some words like \"the\" may be absent in Greek (John 1:1) due to lexical rules\n",
    " - Othertimes a word may become a concept so ngram length may not be the same.\n",
    "\n",
    " \n",
    " Assumptions / Short-cuts to improve on later\n",
    "  - We will ignore capitals for now, later this can be rewritten to allow for capitals as they capture meaning. Ex. (Faith (name) vs faith (concept))\n",
    "  - We will ignore punctiation for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>grams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1197</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>639</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he</td>\n",
       "      <td>398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>eleven</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>faith:</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>languages</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3334</th>\n",
       "      <td>snakes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>poison</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3336 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  frequency  grams\n",
       "0           and       1197      1\n",
       "1           the        942      1\n",
       "2            to        639      1\n",
       "3            of        476      1\n",
       "4            he        398      1\n",
       "...         ...        ...    ...\n",
       "3331     eleven          1      1\n",
       "3332     faith:          1      1\n",
       "3333  languages          1      1\n",
       "3334     snakes          1      1\n",
       "3335     poison          1      1\n",
       "\n",
       "[3336 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_text(text):\n",
    "        # Remove punc and lowercase all words\n",
    "        # TODO: for more languages you can use unicode base tools to look at the type of char it is\n",
    "        # and if type of punc then skip it.\n",
    "        return \" \".join([ word.lower().strip('.,;!?[]{}()\\\\') for word in text.split()])\n",
    "\n",
    "def get_ngrams(df, column_name, max_ngarms_length=10, trim_bigrams=3):\n",
    "    ngrams = defaultdict(int)\n",
    "    for index, row in df.iterrows():\n",
    "        words = normalize_text(row[column_name]).split()\n",
    "        for i, word in enumerate(words):\n",
    "            for j in range(0, max_ngarms_length):\n",
    "                if i+j < len(words):\n",
    "                    ngrams[\" \".join(words[i:i+j+1])] += 1\n",
    "\n",
    "    # Remove any ngrams that have only one count\n",
    "    if trim_bigrams:\n",
    "        ngrams = {ngram: count for ngram, count in ngrams.items() if count >= trim_bigrams or \" \" not in ngram}\n",
    "\n",
    "    # Sort the ngrams by frequency\n",
    "    sorted_ngrams = sorted(ngrams.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_ngrams\n",
    "\n",
    "words = get_ngrams(training_df, 'engBBE')\n",
    "# convert words to dataframe\n",
    "words_df = pd.DataFrame(words, columns=['word', 'frequency'])\n",
    "words_df['grams'] = words_df['word'].apply(lambda x: len(x.split()))\n",
    "\n",
    "\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grams\n",
       "2    1222\n",
       "1    1157\n",
       "3     612\n",
       "4     223\n",
       "5      85\n",
       "6      26\n",
       "7       9\n",
       "8       2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df['grams'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>grams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>and</td>\n",
       "      <td>1197</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>942</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>639</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>of</td>\n",
       "      <td>476</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>he</td>\n",
       "      <td>398</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>eleven</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>faith:</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>languages</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3334</th>\n",
       "      <td>snakes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>poison</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  frequency  grams\n",
       "0           and       1197      1\n",
       "1           the        942      1\n",
       "2            to        639      1\n",
       "3            of        476      1\n",
       "4            he        398      1\n",
       "...         ...        ...    ...\n",
       "3331     eleven          1      1\n",
       "3332     faith:          1      1\n",
       "3333  languages          1      1\n",
       "3334     snakes          1      1\n",
       "3335     poison          1      1\n",
       "\n",
       "[1157 rows x 3 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_df[words_df['grams'] == 1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
