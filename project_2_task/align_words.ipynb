{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align Words across all languages\n",
    "\n",
    "In theory AI supported language translation has 3 parts to it (this notebook dealing with part 1) (at least from a big data perspective)\n",
    " 1. Align all words across all languages so Jesus is Yeshua is Iesu.  All languages have similar word embeddings.\n",
    " 1. Pack all language nuance, commentaries, sermons, translation notes into an embedding representing a verse.\n",
    " 1. Auto-discover language family relationships (as all words come from somewhere) to learn from other related grammers and stemming etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Novel Approach\n",
    "If we consider the case of a concordance, let's say we wanted to know what \"love\" means.  We would thus search the Bible for all verses with love in it.  After reading it we get an impression of what the Bible says/means about love.  \n",
    "\n",
    "Now imagine if we took all those verses in a standarized language that AI understood well (in this case English b/c chatGPT is strongest at that language) and we asked GPT, what is love based on these verses?  In fact we can do that, AI has the concept of a sentence embedding (but sentence can be any text up to 8000 tokens (aprox 6000 words)).  That is it will represent the meaning of all those verses together in a single vector (a list of numbers).\n",
    "\n",
    "If we did this for a second language by looking up every verse reference each word of that language appears in then swapping out the references for the english text (since GPT knows English best) we would get a similar meaning.  Of course in Greek we have Eros, Philia, Agape, etc.  In this case the Greek work Agape would only return the verses using that form of love.  Thus the Greek would have it's own unique embedding for Agape but if you mapped all the words on a graph you should see Greek word Agape sharing similar numbers in space with the English word Love.  This is the goal of this notebook.  That is to get all numbers for similar words to be similar.\n",
    "\n",
    "The alternate approach is you have to know what many words are in each language then get the AI to make them similar in number.  This approach means we do not need to know anything about the language, it is purely stastical and it is leveraging the unique ability that the Bible is aligned in thought verse by verse with the same thoughts shared across all languages for the same verses.\n",
    "\n",
    "Essentially I believe all languages can be reverse engineered.  Further I think it is like a jigsaw puzzle where if you find the easier pieces first (distinct concepts, proper nouns) then the rest of the puzzle becomes easier to solve.  Much like doing the edges first then getting down to the hard pieces last who have but one place to go.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Issues\n",
    "\n",
    "languages like Arabic (so I'm told) where words like \"you\" are embedded in the verb itself.  Thus the verb then is not one word but changes based on the pronoun.  \n",
    "\n",
    "And in Chinese it is based on characters (so I understand).\n",
    "\n",
    "And of course there is idioms and words that do not translate to another word.\n",
    "\n",
    "But we do not want word to word translation.  What we want is a concept in one language is \"close\" to the same concept in another language.  This is represented in vector space.  That is a series of numbers like [1,1,2,1,1,1] is similar to [1,1,2.31,1,1,1] but not to [2,2,1,1,1,2]\n",
    "\n",
    "We want to do this automatically and statistically and this is a well researched technique.  In short we feed an entire corpus like the Bible into a neural network and it learns the vector space.  Then we can take a word like \"green\" and find the closest word like \"blue\" or \"yellow\".  The question remains how to get green to mean \"vert\" (thus aligning it across languages).  We have a unique advantage in that the Bible is chunked into the same verses (essentially but statistically consistently enough).  So we can \"align\" all languages to the same in a unique way.\n",
    "\n",
    "WHY?  If all our words (or more accurately concepts) are aligned with each other than when we say \"For God so loved the world\" we get the same meaning in all languages.  This is the first step to AI supported language.  Without this alignment then we are training each language by itself and are unable to leverage the big data findings we start to get from language families sharing a grammer and nuances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Technical Issues: Small common words with little meaning on own.\n",
    "\n",
    "Because we are grabbing the entire verse there is more meaning in that verse than the word we want.  In the case of common words with little distinct meaning on their own like \"the\", \"his\", \"you\" these will get lost.  I have tried to remediate this by giving them many verses to match with.  Presumably the other language will also have the same verses but in reality many languages use different conjunctions/stems with these words and so they may not align on the same verses.  \n",
    "\n",
    "A potential solution is to consider these some of the inner pieces of a jigsaw puzzle.  We could first resolve the easier words (which we could determine statistically by the nearness to other words) then use the approach in the book _create_glossary) but translate all the known words first, thus leaving the small words which will now have a context of how they are used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Issues\n",
    " - Ambiguity: Many words have multiple meanings, and their usage may differ even within the same document based on the context. It might be challenging to align these types of words across languages effectively.\n",
    " - Non-Biblical Texts: The Bible, while extensive, is a specific type of text with its own set of linguistic features and themes. How well the method generalizes to other kinds of texts is an open question.\n",
    " - Linguistic Drift: Languages evolve over time, and what might be a perfect alignment today may not hold in the future.\n",
    " - If there are minimum verses and usally appear in the same phrase then it may be bound closer to that phrase ex. \"you have heard that it was said but I say to you\" may map \"it was said\" to \"I say to you\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Novel way to Align Languages\n",
    "\n",
    "Since all Bible verses are aligned by verses we can use that alignment to extract a meaning.  To do this we will follow these steps\n",
    "\n",
    "1. Count the occurances of all words in a language (I will show later we will actualy count ngrams to capture meanings but more on this in a moment)\n",
    "1. Starting with the most common word search for all verses that contain that word\n",
    "1. Look up all those verses in a common language (not the source language as openAI sentence embeddings are not aligned the same for each verse across languages - see universal_embeddings.ipynb), it is the common language that will give us a shared meaning of a word across all languages\n",
    "1. Generate a sentence embedding that covers all the verses and thus embeds all the meaning of those verses (this will not give us the meaning of the word but the meaning of that group of verses will be shared with all other languages even when partial as in the case of \"the\" in greek having 16 variations, thus it will be similar but not the same which is good as each langauge has slighly different nuances per word)\n",
    "\n",
    "A potential extension of this work\n",
    "1. Take FastText and all Bibles across all languages train new embeddings\n",
    "1. Using the embeddings above as pre-trained embeddings\n",
    "1. Thus deepening the meaning of each word to actually be the words meaning for that language but with a starting point of it's shared meaning as used in verses across all languages.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some defaults\n",
    "TRAINING_SOURCE = ['MAT',\n",
    " 'MRK',\n",
    " 'LUK',\n",
    " 'JHN',\n",
    " 'ACT',\n",
    " 'ROM',\n",
    " '1CO',\n",
    " '2CO',\n",
    " 'GAL',\n",
    " 'EPH',\n",
    " 'PHP',\n",
    " 'COL',\n",
    " '1TH',\n",
    " '2TH',\n",
    " '1TI',\n",
    " '2TI',\n",
    " 'TIT',\n",
    " 'PHM',\n",
    " 'HEB',\n",
    " 'JAS',\n",
    " '1PE',\n",
    " '2PE',\n",
    " '1JN',\n",
    " '2JN',\n",
    " '3JN',\n",
    " 'JUD',\n",
    " 'REV'\n",
    " ]\n",
    "VERSIONS = ['eng-web','eng-asv','eng-kjv2006','engBBE','hin2017', 'arbnav', 'latVUC', 'amo']\n",
    "COMMON_VERSION = 'eng-asv'\n",
    "MAX_VERSES = 50\n",
    "MODEL = 'text-embedding-ada-002'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and fixes to pathing\n",
    "%reload_ext autoreload\n",
    "import sys\n",
    "sys.path.append('../lib')\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import openai, time, os\n",
    "from openai.error import RateLimitError, OpenAIError\n",
    "from config import get_config\n",
    "from collections import defaultdict\n",
    "from cipher import substitution_cipher\n",
    "\n",
    "# openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = get_config('openai')['api_type']\n",
    "# openai.api_base = os.environ[\"OPENAI_API_BASE\"] = get_config('openai')['api_base']\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"] = get_config('openai')['api_key']\n",
    "# openai.api_version = os.environ[\"OPENAI_API_VERSION\"] = get_config('openai')['api_version']\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"] = get_config('embeddings')['api_key']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get our Bibles\n",
    "\n",
    "https://github.com/ChrisPriebe/BibleTranslation/blob/master/get_bible.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vref</th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>verse</th>\n",
       "      <th>eng-web</th>\n",
       "      <th>eng-asv</th>\n",
       "      <th>eng-kjv2006</th>\n",
       "      <th>engBBE</th>\n",
       "      <th>hin2017</th>\n",
       "      <th>arbnav</th>\n",
       "      <th>latVUC</th>\n",
       "      <th>amo</th>\n",
       "      <th>source_content</th>\n",
       "      <th>birrig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GEN 1:1</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the beginning, God created the heavens and ...</td>\n",
       "      <td>In the beginning God created the heavens and t...</td>\n",
       "      <td>In the beginning God created the heaven and th...</td>\n",
       "      <td>At the first God made the heaven and the earth.</td>\n",
       "      <td>आदि में परमेश्‍वर ने आकाश और पृथ्वी की सृष्टि ...</td>\n",
       "      <td>فِي الْبَدْءِ خَلَقَ اللهُ السَّمَاوَاتِ وَالأ...</td>\n",
       "      <td>In principio creavit Deus cælum et terram.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>בְּרֵאשִׁ֖ית  בָּרָ֣א  אֱלֹהִ֑ים  אֵ֥ת  הַשָּׁ...</td>\n",
       "      <td>El lxi sovzl Guw newi lxi xiemir erw lxi ievlx.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEN 1:2</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The earth was formless and empty. Darkness was...</td>\n",
       "      <td>And the earth was waste and void; and darkness...</td>\n",
       "      <td>And the earth was without form, and void; and ...</td>\n",
       "      <td>And the earth was waste and without form; and ...</td>\n",
       "      <td>पृथ्वी बेडौल और सुनसान पड़ी थी, और गहरे जल के ...</td>\n",
       "      <td>وَإِذْ كَانَتِ الأَرْضُ مُشَوَّشَةً وَمُقْفِرَ...</td>\n",
       "      <td>Terra autem erat inanis et vacua, et tenebræ e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וְהָאָ֗רֶץ  הָיְתָ֥ה  תֹ֨הוּ֙  וָבֹ֔הוּ  וְחֹ֖...</td>\n",
       "      <td>Erw lxi ievlx hez hezli erw holxual suvn; erw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEN 1:3</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>God said, “Let there be light,” and there was ...</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>तब परमेश्‍वर ने कहा, “उजियाला हो*,” तो उजियाला...</td>\n",
       "      <td>أَمَرَ اللهُ: «لِيَكُنْ نُورٌ». فَصَارَ نُورٌ،</td>\n",
       "      <td>Dixitque Deus: Fiat lux. Et facta est lux.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וַיֹּ֥אמֶר  אֱלֹהִ֖ים  יְהִ֣י  א֑וֹר  וַֽיְהִי...</td>\n",
       "      <td>Erw Guw zeow, Pil lxivi fi pogxl: erw lxivi he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GEN 1:4</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>God saw the light, and saw that it was good. G...</td>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>And God, looking on the light, saw that it was...</td>\n",
       "      <td>और परमेश्‍वर ने उजियाले को देखा कि अच्छा है*; ...</td>\n",
       "      <td>وَرَأَى اللهُ النُّورَ فَاسْتَحْسَنَهُ وَفَصَل...</td>\n",
       "      <td>Et vidit Deus lucem quod esset bona: et divisi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וַיַּ֧רְא  אֱלֹהִ֛ים  אֶת־ הָא֖וֹר  כִּי־ ט֑וֹ...</td>\n",
       "      <td>Erw Guw, puucorg ur lxi pogxl, zeh lxel ol hez...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GEN 1:5</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>God called the light “day”, and the darkness h...</td>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>Naming the light, Day, and the dark, Night. An...</td>\n",
       "      <td>और परमेश्‍वर ने उजियाले को दिन और अंधियारे को ...</td>\n",
       "      <td>وَسَمَّى اللهُ النُّورَ نَهَاراً، أَمَّا الظَّ...</td>\n",
       "      <td>Appellavitque lucem Diem, et tenebras Noctem: ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וַיִּקְרָ֨א  אֱלֹהִ֤ים׀ לָאוֹר֙  י֔וֹם  וְלַחֹ...</td>\n",
       "      <td>Renorg lxi pogxl, Wej, erw lxi wevc, Rogxl. Er...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      vref book  chapter  verse  \\\n",
       "0  GEN 1:1  GEN        1      1   \n",
       "1  GEN 1:2  GEN        1      2   \n",
       "2  GEN 1:3  GEN        1      3   \n",
       "3  GEN 1:4  GEN        1      4   \n",
       "4  GEN 1:5  GEN        1      5   \n",
       "\n",
       "                                             eng-web  \\\n",
       "0  In the beginning, God created the heavens and ...   \n",
       "1  The earth was formless and empty. Darkness was...   \n",
       "2  God said, “Let there be light,” and there was ...   \n",
       "3  God saw the light, and saw that it was good. G...   \n",
       "4  God called the light “day”, and the darkness h...   \n",
       "\n",
       "                                             eng-asv  \\\n",
       "0  In the beginning God created the heavens and t...   \n",
       "1  And the earth was waste and void; and darkness...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                         eng-kjv2006  \\\n",
       "0  In the beginning God created the heaven and th...   \n",
       "1  And the earth was without form, and void; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                              engBBE  \\\n",
       "0    At the first God made the heaven and the earth.   \n",
       "1  And the earth was waste and without form; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God, looking on the light, saw that it was...   \n",
       "4  Naming the light, Day, and the dark, Night. An...   \n",
       "\n",
       "                                             hin2017  \\\n",
       "0  आदि में परमेश्‍वर ने आकाश और पृथ्वी की सृष्टि ...   \n",
       "1  पृथ्वी बेडौल और सुनसान पड़ी थी, और गहरे जल के ...   \n",
       "2  तब परमेश्‍वर ने कहा, “उजियाला हो*,” तो उजियाला...   \n",
       "3  और परमेश्‍वर ने उजियाले को देखा कि अच्छा है*; ...   \n",
       "4  और परमेश्‍वर ने उजियाले को दिन और अंधियारे को ...   \n",
       "\n",
       "                                              arbnav  \\\n",
       "0  فِي الْبَدْءِ خَلَقَ اللهُ السَّمَاوَاتِ وَالأ...   \n",
       "1  وَإِذْ كَانَتِ الأَرْضُ مُشَوَّشَةً وَمُقْفِرَ...   \n",
       "2     أَمَرَ اللهُ: «لِيَكُنْ نُورٌ». فَصَارَ نُورٌ،   \n",
       "3  وَرَأَى اللهُ النُّورَ فَاسْتَحْسَنَهُ وَفَصَل...   \n",
       "4  وَسَمَّى اللهُ النُّورَ نَهَاراً، أَمَّا الظَّ...   \n",
       "\n",
       "                                              latVUC  amo  \\\n",
       "0         In principio creavit Deus cælum et terram.  NaN   \n",
       "1  Terra autem erat inanis et vacua, et tenebræ e...  NaN   \n",
       "2         Dixitque Deus: Fiat lux. Et facta est lux.  NaN   \n",
       "3  Et vidit Deus lucem quod esset bona: et divisi...  NaN   \n",
       "4  Appellavitque lucem Diem, et tenebras Noctem: ...  NaN   \n",
       "\n",
       "                                      source_content  \\\n",
       "0  בְּרֵאשִׁ֖ית  בָּרָ֣א  אֱלֹהִ֑ים  אֵ֥ת  הַשָּׁ...   \n",
       "1  וְהָאָ֗רֶץ  הָיְתָ֥ה  תֹ֨הוּ֙  וָבֹ֔הוּ  וְחֹ֖...   \n",
       "2  וַיֹּ֥אמֶר  אֱלֹהִ֖ים  יְהִ֣י  א֑וֹר  וַֽיְהִי...   \n",
       "3  וַיַּ֧רְא  אֱלֹהִ֛ים  אֶת־ הָא֖וֹר  כִּי־ ט֑וֹ...   \n",
       "4  וַיִּקְרָ֨א  אֱלֹהִ֤ים׀ לָאוֹר֙  י֔וֹם  וְלַחֹ...   \n",
       "\n",
       "                                              birrig  \n",
       "0    El lxi sovzl Guw newi lxi xiemir erw lxi ievlx.  \n",
       "1  Erw lxi ievlx hez hezli erw holxual suvn; erw ...  \n",
       "2  Erw Guw zeow, Pil lxivi fi pogxl: erw lxivi he...  \n",
       "3  Erw Guw, puucorg ur lxi pogxl, zeh lxel ol hez...  \n",
       "4  Renorg lxi pogxl, Wej, erw lxi wevc, Rogxl. Er...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: remplace this with download from ecorpus and prepare it\n",
    "\n",
    "# Read the data/berrig.csv file into dataframe\n",
    "df = pd.read_csv('../data/birrig.csv')\n",
    "df.rename(columns={df.columns[0]: 'vref'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE: 7957 verses\n"
     ]
    }
   ],
   "source": [
    "training_df = df[df['book'].isin(TRAINING_SOURCE)]\n",
    "print(f\"TRAIN SIZE: {len(training_df)} verses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "        # Remove punc and lowercase all words\n",
    "        # TODO: for more languages you can use unicode base tools to look at the type of char it is\n",
    "        # and if type of punc then skip it.\n",
    "        return \" \".join([ word.lower().strip('.,;!?[]{}()\\'\"\\\\') for word in text.split()])\n",
    "\n",
    "\n",
    "def get_ngrams(df, column_name, max_ngarms_length=10, trim_bigrams=5, normalize=False):\n",
    "    ngrams = defaultdict(int)\n",
    "    for index, row in df.iterrows():\n",
    "        text = normalize_text and normalize_text(row[column_name]) or row[column_name]\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            for j in range(0, max_ngarms_length):\n",
    "                if i+j < len(words):\n",
    "                    ngrams[\" \".join(words[i:i+j+1])] += 1\n",
    "\n",
    "    # Remove any ngrams that have only one count\n",
    "    if trim_bigrams:\n",
    "        ngrams = {ngram: count for ngram, count in ngrams.items() if count >= trim_bigrams or \" \" not in ngram}\n",
    "\n",
    "    # Sort the ngrams by frequency\n",
    "    sorted_ngrams = sorted(ngrams.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_ngrams\n",
    "\n",
    "def get_embedding(inputs, words_df):\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            result = openai.Embedding.create(input=[input for (_, input) in inputs], model=MODEL)['data']\n",
    "            for i, (word, _) in enumerate(inputs):\n",
    "                embeddings.loc[len(embeddings)] = [word['word'], result[i]['embedding']]\n",
    "            return\n",
    "        \n",
    "        except RateLimitError as e:\n",
    "            print(f\"Rate Limit Error: {e}\")\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"502 Bad Gateway\" in e.message:\n",
    "                print(\"Bad Gateway Error\")\n",
    "                time.sleep(30)\n",
    "                continue\n",
    "\n",
    "            print(\"Error\", e)\n",
    "            return\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "for VERSION in VERSIONS:\n",
    "    print(\"Processing\", VERSION)\n",
    "    # remove nan from training_df[VERSION]\n",
    "    df = training_df[training_df[VERSION].notna()]\n",
    "    df = df[df[COMMON_VERSION].notna()]\n",
    "    df.loc[:, 'normalized'] = df[VERSION].apply(normalize_text)\n",
    "\n",
    "    words = get_ngrams(df, 'normalized')\n",
    "    # convert words to dataframe\n",
    "    words_df = pd.DataFrame(words, columns=['word', 'frequency'])\n",
    "    words_df['grams'] = words_df['word'].apply(lambda x: len(x.split()))\n",
    "    # limit to words that appear at least 3 times\n",
    "    words_df = words_df[words_df['frequency'] > 2]\n",
    "\n",
    "    print(\"Total Words\", len(words_df))\n",
    "\n",
    "    current_tokens = 0\n",
    "    inputs = []\n",
    "    embeddings = pd.DataFrame(columns=['word', 'embeddings'])\n",
    "    total_tokens = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Create our sentence embedding\n",
    "    for _, word in words_df.iterrows():\n",
    "        # Find all the occurrences of the word in the training data\n",
    "        verses = df[df['normalized'].str.contains(word['word'])][['vref', COMMON_VERSION]]\n",
    "\n",
    "        # Create the input for the embedding\n",
    "        keep = len(verses) > MAX_VERSES and MAX_VERSES / len(verses) or 1.0\n",
    "        while input is None or len(tokenizer.encode(input)) > 7000:\n",
    "            # Cut the input by 20% until it's under 8000 tokens\n",
    "            #input = '\\n'.join([f\"{verse['vref']}\\t{verse[COMMON_VERSION]}\" for _, verse in verses.sample(frac=keep, random_state=1).iterrows()])\n",
    "            input = '\\n'.join([verse[COMMON_VERSION] for _, verse in verses.sample(frac=keep, random_state=1).iterrows()])\n",
    "            keep = keep * 0.8\n",
    "\n",
    "        current_tokens += len(tokenizer.encode(input))\n",
    "        if current_tokens > 7000:\n",
    "            total % 100 == 0 and print(f\"Getting Embeddings\\t {total} of {len(words_df)} {total/len(words_df)}%\")\n",
    "            get_embedding(inputs, words_df)\n",
    "            total_tokens += current_tokens\n",
    "            current_tokens = len(tokenizer.encode(input))\n",
    "            inputs = []\n",
    "\n",
    "    \n",
    "        inputs.append((word, input))\n",
    "        total += 1\n",
    "        input = None\n",
    "\n",
    "    # One last call to finish it off\n",
    "    get_embedding(inputs, words_df)\n",
    "    total_tokens += current_tokens\n",
    "\n",
    "    print(f\"DONE {VERSION}\\ttotal tokens {total_tokens}\\tAprox Cost ${total_tokens*0.0000001}\")\n",
    "\n",
    "    words_df['version'] = VERSION\n",
    "    # join the embeddings to the words_df on word\n",
    "    df2 = words_df.merge(embeddings, on='word', how='left')\n",
    "    df2.to_json(f'../data/{VERSION}_embeddings.jsonl', orient='records', lines=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68466"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO \n",
    "# compare embeddings to see which are matched\n",
    "# test with other languages (like the greek the issue where has many forms)\n",
    "\n",
    "# load the embeddings and concat them\n",
    "df = pd.concat([\n",
    "    pd.read_json('../data/eng-web_embeddings.jsonl', lines=True),\\\n",
    "    #pd.read_json('../data/birrig_embeddings.jsonl', lines=True),\\\n",
    "    pd.read_json('../data/engBBE_embeddings.jsonl', lines=True), \\\n",
    "    pd.read_json('../data/eng-asv_embeddings.jsonl', lines=True), \\\n",
    "    pd.read_json('../data/hin2017_embeddings.jsonl', lines=True), \\\n",
    "    pd.read_json('../data/latVUC_embeddings.jsonl', lines=True), \\\n",
    "    pd.read_json('../data/arbnav_embeddings.jsonl', lines=True), \\\n",
    "    pd.read_json('../data/source_content_embeddings.jsonl', lines=True), \\\n",
    "])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column embedding if it exists\n",
    "df.drop(columns=['embedding'], inplace=True, errors='ignore')\n",
    "\n",
    "# drop rows with no value in embeddings\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'amen' from eng-web which is most similar to\n",
      "                    word  version  grams\n",
      "1000                amen  eng-web      1\n",
      "1987               be it   engBBE      2\n",
      "12472               आमीन  hin2017      1\n",
      "2259         be with you  eng-asv      3\n",
      "1885               so be   engBBE      2\n",
      "1496               आमीन।  hin2017      1\n",
      "2925   مَعَكُمْ جَمِيعاً   arbnav      2\n",
      "11865  with you all amen  eng-web      4\n",
      "11866       you all amen  eng-web      3\n",
      "11867           all amen  eng-web      2\n",
      "Processing 'of life' from eng-web which is most similar to\n",
      "                   word  version  grams\n",
      "1001            of life  eng-web      2\n",
      "1061            of life  eng-asv      2\n",
      "1564           i am the  eng-asv      3\n",
      "4716          “i am the  eng-web      3\n",
      "2637       believeth on  eng-asv      2\n",
      "4412   who has faith in   engBBE      4\n",
      "2503           the life  eng-asv      2\n",
      "3390     मुझ पर विश्वास  hin2017      3\n",
      "4404  that believeth on  eng-asv      3\n",
      "4302       जीवन के लिये  hin2017      3\n",
      "Processing 'is he' from eng-web which is most similar to\n",
      "                   word  version  grams\n",
      "1002              is he  eng-web      2\n",
      "759               is he  eng-asv      2\n",
      "6619            has now   engBBE      2\n",
      "364           and jesus   engBBE      2\n",
      "2440           his head   engBBE      2\n",
      "1305  jesus said to him   engBBE      4\n",
      "1630              is he   engBBE      2\n",
      "4275           رَأْسَهُ   arbnav      1\n",
      "1856            this he  eng-asv      2\n",
      "9708           जगह नहीं  hin2017      2\n",
      "Processing 'the angel' from eng-web which is most similar to\n",
      "                  word  version  grams\n",
      "1003         the angel  eng-web      2\n",
      "5555      के स्वर्गदूत  hin2017      2\n",
      "1363         the angel   engBBE      2\n",
      "333          स्वर्गदूत  hin2017      1\n",
      "392              angel  eng-web      1\n",
      "1095         the angel  eng-asv      2\n",
      "8054      to the angel  eng-asv      3\n",
      "6422      to the angel   engBBE      3\n",
      "5299  the angel of the   engBBE      4\n",
      "4851      the angel of   engBBE      3\n",
      "Processing 'when jesus' from eng-web which is most similar to\n",
      "                 word  version  grams\n",
      "1004       when jesus  eng-web      2\n",
      "1169       when jesus  eng-asv      2\n",
      "7650  had come to the   engBBE      4\n",
      "1555          जब यीशु  hin2017      2\n",
      "2098        jesus had  eng-asv      2\n",
      "2561      had come to   engBBE      3\n",
      "1598       when jesus   engBBE      2\n",
      "8898   jesus had come   engBBE      3\n",
      "4923   when jesus had   engBBE      3\n",
      "1830      come to the   engBBE      3\n",
      "Processing 'the days' from eng-web which is most similar to\n",
      "             word  version  grams\n",
      "1005     the days  eng-web      2\n",
      "996      the days  eng-asv      2\n",
      "1948     the days   engBBE      2\n",
      "1579      days of  eng-web      2\n",
      "1442   those days  eng-web      2\n",
      "1650  the days of  eng-web      3\n",
      "3633       वे दिन  hin2017      2\n",
      "1649      days of  eng-asv      2\n",
      "3634        आएँगे  hin2017      1\n",
      "3298      days of   engBBE      2\n",
      "Processing 'for his' from eng-web which is most similar to\n",
      "             word  version  grams\n",
      "1006      for his  eng-web      2\n",
      "10621      or his  eng-asv      2\n",
      "10092      or his  eng-web      2\n",
      "1379      for his  eng-asv      2\n",
      "1006   हमारे लिये  hin2017      2\n",
      "772      he might  eng-asv      2\n",
      "798     free from   engBBE      2\n",
      "8             for  eng-web      1\n",
      "3373      us that  eng-web      2\n",
      "62         لِكَيْ   arbnav      1\n",
      "Processing 'led' from eng-web which is most similar to\n",
      "                  word  version  grams\n",
      "1007               led  eng-web      1\n",
      "357                 कह  hin2017      1\n",
      "23                  it  eng-web      1\n",
      "43                  me  eng-web      1\n",
      "376              as he  eng-asv      2\n",
      "2530   और परमेश्‍वर का  hin2017      3\n",
      "12255              सकी  hin2017      1\n",
      "3522          ظَهَرَتْ   arbnav      1\n",
      "1549            at his   engBBE      2\n",
      "65             because   engBBE      1\n",
      "Processing 'and said to him' from eng-web which is most similar to\n",
      "                   word  version  grams\n",
      "1008    and said to him  eng-web      4\n",
      "608         قَالَ لَهُ:   arbnav      2\n",
      "353         and said to  eng-web      3\n",
      "823   and said unto him  eng-asv      4\n",
      "594         and said to   engBBE      3\n",
      "2330         से कहा “हे  hin2017      3\n",
      "596       فَقَالَ لَهُ:   arbnav      2\n",
      "185            and said  eng-web      2\n",
      "3263       पास आकर उससे  hin2017      3\n",
      "2060       him and said  eng-web      3\n",
      "Processing 'the boat' from eng-web which is most similar to\n",
      "               word  version  grams\n",
      "1009       the boat  eng-web      2\n",
      "1248       the boat  eng-asv      2\n",
      "787            boat  eng-web      1\n",
      "901            boat  eng-asv      1\n",
      "1010           boat   engBBE      1\n",
      "1299         नाव पर  hin2017      2\n",
      "3368       got into   engBBE      2\n",
      "2873       boat and  eng-asv      2\n",
      "4184  into the boat  eng-asv      3\n",
      "3033  into the boat  eng-web      3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from openai.embeddings_utils import cosine_similarity\n",
    "# convert the DataFrame to a matrix\n",
    "\n",
    "for _, word in df[1000:1010].iterrows():\n",
    "    print(f\"Processing '{word['word']}' from {word['version']} which is most similar to\")\n",
    "    # Remove this verse so we don't get ourselves\n",
    "    df['similarities'] = df.embeddings.apply(lambda x: cosine_similarity(x, word['embeddings']))\n",
    "    print(df.sort_values('similarities', ascending=False)[['word','version', 'grams']].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test some specific words, created with a GPT prompt\n",
    "tests = {\n",
    "  \"love\": {\n",
    "    \"Arabic\": [\"حب\"],\n",
    "    \"Latin\": [\"amor\"],\n",
    "    \"Hindi\": [\"प्रेम\", \"मोहब्बत\"],\n",
    "    \"Greek\": [\"ἀγάπη\", \"έρως\"]\n",
    "  },\n",
    "  \"peace\": {\n",
    "    \"Arabic\": [\"سلام\"],\n",
    "    \"Latin\": [\"pax\"],\n",
    "    \"Hindi\": [\"शांति\", \"अमन\"],\n",
    "    \"Greek\": [\"εἰρήνη\"]\n",
    "  },\n",
    "  \"Jesus\": {\n",
    "    \"Arabic\": [\"يسوع\"],\n",
    "    \"Latin\": [\"Iesus\"],\n",
    "    \"Hindi\": [\"ईसा\"],\n",
    "    \"Greek\": [\"Ἰησοῦς\"]\n",
    "  },\n",
    "  \"Moses\": {\n",
    "    \"Arabic\": [\"موسى\"],\n",
    "    \"Latin\": [\"Moses\"],\n",
    "    \"Hindi\": [\"मूसा\"],\n",
    "    \"Greek\": [\"Μωυσῆς\"]\n",
    "  },\n",
    "  \"meek\": {\n",
    "    \"Arabic\": [\"وديع\", \"خاضع\"],\n",
    "    \"Latin\": [\"mitis\"],\n",
    "    \"Hindi\": [\"विनम्र\", \"कोमल\"],\n",
    "    \"Greek\": [\"πρᾳός\"]\n",
    "  },\n",
    "  \"slave\": {\n",
    "    \"Arabic\": [\"عبد\", \"رقيق\"],\n",
    "    \"Latin\": [\"servus\"],\n",
    "    \"Hindi\": [\"गुलाम\", \"दास\"],\n",
    "    \"Greek\": [\"δοῦλος\"]\n",
    "  },\n",
    "  \"the\": {\n",
    "    \"Arabic\": [\"ال\"],\n",
    "    \"Latin\": [\"ille\", \"illa\", \"illud\"],\n",
    "    \"Hindi\": [\"यह\", \"वह\"],\n",
    "    \"Greek\": [\"ὁ\", \"ἡ\", \"τό\"]\n",
    "  },\n",
    "  \"slave to sin\": {\n",
    "    \"Arabic\": [\"عبد للخطيئة\"],\n",
    "    \"Latin\": [\"servus peccati\"],\n",
    "    \"Hindi\": [\"पाप का गुलाम\"],\n",
    "    \"Greek\": [\"δοῦλος τῆς ἁμαρτίας\"]\n",
    "  },\n",
    "  \"he said\": {\n",
    "    \"Arabic\": [\"قال\"],\n",
    "    \"Latin\": [\"dixit\"],\n",
    "    \"Hindi\": [\"उसने कहा\"],\n",
    "    \"Greek\": [\"εἶπεν\"]\n",
    "  },\n",
    "  \"then I saw\": {\n",
    "    \"Arabic\": [\"ثم رأيت\"],\n",
    "    \"Latin\": [\"tum vidi\"],\n",
    "    \"Hindi\": [\"फिर मैंने देखा\"],\n",
    "    \"Greek\": [\"τότε εἶδον\"]\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing love from eng-web that appeared 205 times\n",
      "  Arabic\tحب not found\n",
      "  Latin\tamor not found\n",
      "  Hindi\tप्रेम\t0.9248584900873895\n",
      "  Hindi\tमोहब्बत not found\n",
      "  Greek\tἀγάπη not found\n",
      "  Greek\tέρως not found\n",
      "                         word  version  similarities\n",
      "156                      love  eng-web      1.000000\n",
      "878                     loved  eng-web      0.945976\n",
      "806             الأَحِبَّاءُ،   arbnav      0.945891\n",
      "805    أَيُّهَا الأَحِبَّاءُ،   arbnav      0.945891\n",
      "2281                  प्रियों  hin2017      0.945433\n",
      "1037                    loved  eng-asv      0.941613\n",
      "2719                 my loved   engBBE      0.938760\n",
      "5560                تُحِبُّوا   arbnav      0.937771\n",
      "1722                  for one   engBBE      0.937285\n",
      "11915                हे प्रिय  hin2017      0.937039\n",
      "Testing peace from eng-web that appeared 86 times\n",
      "  Arabic\tسلام not found\n",
      "  Latin\tpax not found\n",
      "  Hindi\tशांति not found\n",
      "  Hindi\tअमन not found\n",
      "  Greek\tεἰρήνη not found\n",
      "                word  version  similarities\n",
      "406            peace  eng-web      1.000000\n",
      "338            peace  eng-asv      0.964966\n",
      "429           शान्ति  hin2017      0.963273\n",
      "2183       and peace  eng-asv      0.958209\n",
      "7638           शान्त  hin2017      0.952498\n",
      "441            peace   engBBE      0.952051\n",
      "2956       and peace   engBBE      0.950475\n",
      "2509  father and the  eng-web      0.949461\n",
      "2937  father and the  eng-asv      0.949149\n",
      "5848         may the   engBBE      0.949053\n",
      "Testing jesus from eng-web that appeared 934 times\n",
      "  Arabic\tيسوع not found\n",
      "  Latin\tiesus not found\n",
      "  Hindi\tईसा not found\n",
      "  Greek\tἰησοῦς\t0.9274787948530561\n",
      "           word  version  similarities\n",
      "32        jesus  eng-web      1.000000\n",
      "508     «أَنْتَ   arbnav      0.957040\n",
      "10649     he to   engBBE      0.949947\n",
      "93      यीशु ने  hin2017      0.946906\n",
      "310        “you  eng-web      0.944746\n",
      "266      قَالَ:   arbnav      0.943520\n",
      "1292   to jesus  eng-web      0.942887\n",
      "1192   to jesus   engBBE      0.942451\n",
      "132    وَقَالَ:   arbnav      0.942262\n",
      "3094     at him  eng-web      0.942072\n",
      "Testing moses from eng-web that appeared 75 times\n",
      "  Arabic\tموسى not found\n",
      "  Latin\tmoses\t1.0\n",
      "  Hindi\tमूसा\t1.0\n",
      "  Greek\tμωυσῆς not found\n",
      "           word  version  similarities\n",
      "493       moses  eng-asv      1.000000\n",
      "489        मूसा  hin2017      1.000000\n",
      "634       moses   engBBE      1.000000\n",
      "472       moses  eng-web      1.000000\n",
      "281      مُوسَى   arbnav      0.969721\n",
      "649     مُوسَى،   arbnav      0.948944\n",
      "1852    मूसा ने  hin2017      0.946742\n",
      "3239   of moses   engBBE      0.946117\n",
      "11337   कि मूसा  hin2017      0.935251\n",
      "48       وَقَدْ   arbnav      0.934991\n",
      "Testing meek from eng-asv that appeared 3 times\n",
      "  Arabic\tوديع not found\n",
      "  Arabic\tخاضع not found\n",
      "  Latin\tmitis not found\n",
      "  Hindi\tविनम्र not found\n",
      "  Hindi\tकोमल\t0.8763727248502003\n",
      "  Greek\tπρᾳός not found\n",
      "               word  version  similarities\n",
      "13260          meek  eng-asv      1.000000\n",
      "5024       meekness  eng-asv      0.971235\n",
      "3540         नम्रता  hin2017      0.965269\n",
      "4887         gentle  eng-web      0.964271\n",
      "11880     नम्रता और  hin2017      0.962347\n",
      "9542           नम्र  hin2017      0.953666\n",
      "11981  humility and  eng-web      0.950339\n",
      "6376     gentleness  eng-web      0.949883\n",
      "2703         gentle   engBBE      0.948355\n",
      "4764       humility  eng-web      0.947841\n",
      "Testing slave from eng-web that appeared 6 times\n",
      "  Arabic\tعبد not found\n",
      "  Arabic\tرقيق not found\n",
      "  Latin\tservus not found\n",
      "  Hindi\tगुलाम not found\n",
      "  Hindi\tदास\t0.8796391254714815\n",
      "  Greek\tδοῦλος not found\n",
      "                  word  version  similarities\n",
      "9318             slave  eng-web      1.000000\n",
      "10006       of all men   engBBE      0.922654\n",
      "11045         free and  eng-web      0.922026\n",
      "9863         kings and   engBBE      0.919964\n",
      "1514              माँस  hin2017      0.911379\n",
      "495            may not   engBBE      0.911126\n",
      "12090     the flesh of   engBBE      0.910351\n",
      "12343        small and  eng-asv      0.910199\n",
      "15702  small and great   engBBE      0.910199\n",
      "15701        small and   engBBE      0.910114\n",
      "Testing the from eng-web that appeared 10884 times\n",
      "  Arabic\tال not found\n",
      "  Latin\tille not found\n",
      "  Latin\tilla not found\n",
      "  Latin\tillud not found\n",
      "  Hindi\tयह\t0.8774424054749568\n",
      "  Hindi\tवह\t0.8950747903480486\n",
      "  Greek\tὁ\t0.8932370347167097\n",
      "  Greek\tἡ\t0.9054809266488539\n",
      "  Greek\tτό not found\n",
      "            word  version  similarities\n",
      "0            the  eng-web      1.000000\n",
      "416         hour  eng-web      1.000000\n",
      "374         hour  eng-asv      0.963575\n",
      "2607    the hour  eng-web      0.963159\n",
      "660         hour   engBBE      0.958582\n",
      "1405  السَّاعَةُ   arbnav      0.953802\n",
      "409   السَّاعَةِ   arbnav      0.950489\n",
      "1021        घड़ी  hin2017      0.949568\n",
      "2040    the hour  eng-asv      0.946543\n",
      "4210      पहुँची  hin2017      0.945507\n",
      "slave to sin not found in dataframe\n",
      "Testing he said from eng-web that appeared 314 times\n",
      "  Arabic\tقال not found\n",
      "  Latin\tdixit not found\n",
      "  Hindi\tउसने कहा\t0.9260262086319121\n",
      "  Greek\tεἶπεν\t0.9337141553209629\n",
      "                 word  version  similarities\n",
      "96            he said  eng-web      1.000000\n",
      "59             to him  eng-web      0.954743\n",
      "299   he said to them  eng-web      0.952198\n",
      "351    and he said to   engBBE      0.951304\n",
      "1448            से कह  hin2017      0.949982\n",
      "48            said to  eng-web      0.949821\n",
      "414   he said to them   engBBE      0.949301\n",
      "201        he said to   engBBE      0.948832\n",
      "718    فَقَالَ لَهُمْ   arbnav      0.948559\n",
      "141          उनसे कहा  hin2017      0.948474\n",
      "then i saw not found in dataframe\n"
     ]
    }
   ],
   "source": [
    "# Loop through each word in tests and for each language look up that word and compare the embeddings of each\n",
    "for word, translations in tests.items():\n",
    "    # Find the word in the dataframe\n",
    "    word = normalize_text(word)\n",
    "    word_df = df[df['word'] == word]\n",
    "    if len(word_df) == 0:\n",
    "        print(f\"{word} not found in dataframe\")\n",
    "        continue\n",
    "    word = word_df.iloc[0]\n",
    "    print(f\"Testing {word['word']} from {word['version']} that appeared {word['frequency']} times\")\n",
    "\n",
    "    for language, translations in translations.items():\n",
    "        for translation in translations:\n",
    "            # Find the word in the dataframe\n",
    "            translation = normalize_text(translation)\n",
    "            df2 = df[df['word'] == translation]\n",
    "            if len(df2) == 0:\n",
    "                print(f\"  {language}\\t{translation} not found\")\n",
    "                continue\n",
    "\n",
    "            # Get the cosine similarity between word and translation\n",
    "            similarity = cosine_similarity(word['embeddings'], df2.iloc[0]['embeddings'])\n",
    "            print(f\"  {language}\\t{translation}\\t{similarity}\")\n",
    "        \n",
    "    df['similarities'] = df.embeddings.apply(lambda x: cosine_similarity(x, word['embeddings']))\n",
    "    print(df.sort_values('similarities', ascending=False)[['word','version', 'similarities']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    " - [ ] try sorting matching verses by length of verse (shortest first) to see if that helps\n",
    " - [/] confirm sample with random_state keeps randomizer current\n",
    " - [ ] try matching more verses\n",
    " - [ ] try different versions like amplified which has more words\n",
    " - [ ] Would fasttext sentence embeddings be better starting with a an already "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
