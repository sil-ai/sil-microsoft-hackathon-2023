{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Align Words across all languages\n",
    "\n",
    "In theory AI supported language translation has 3 parts to it (this notebook dealing with part 1) (at least from a big data perspective)\n",
    " 1. Align all words across all languages so Jesus is Yeshua is Iesu.  All languages have similar word embeddings.\n",
    " 1. Pack all language nuance, commentaries, sermons, translation notes into an embedding representing a verse.\n",
    " 1. Auto-discover language family relationships (as all words come from somewhere) to learn from other related grammers and stemming etc.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Novel Approach\n",
    "If we consider the case of a concordance, let's say we wanted to know what \"love\" means.  We would thus search the Bible for all verses with love in it.  After reading it we get an impression of what the Bible says/means about love.  \n",
    "\n",
    "Now imagine if we took all those verses in a standarized language that AI understood well (in this case English b/c chatGPT is strongest at that language) and we asked GPT, what is love based on these verses?  In fact we can do that, AI has the concept of a sentence embedding (but sentence can be any text up to 8000 tokens (aprox 6000 words)).  That is it will represent the meaning of all those verses together in a single vector (a list of numbers).\n",
    "\n",
    "If we did this for a second language by looking up every verse reference each word of that language appears in then swapping out the references for the english text (since GPT knows English best) we would get a similar meaning.  Of course in Greek we have Eros, Philia, Agape, etc.  In this case the Greek work Agape would only return the verses using that form of love.  Thus the Greek would have it's own unique embedding for Agape but if you mapped all the words on a graph you should see Greek word Agape sharing similar numbers in space with the English word Love.  This is the goal of this notebook.  That is to get all numbers for similar words to be similar.\n",
    "\n",
    "The alternate approach is you have to know what many words are in each language then get the AI to make them similar in number.  This approach means we do not need to know anything about the language, it is purely stastical and it is leveraging the unique ability that the Bible is aligned in thought verse by verse with the same thoughts shared across all languages for the same verses.\n",
    "\n",
    "Essentially I believe all languages can be reverse engineered.  Further I think it is like a jigsaw puzzle where if you find the easier pieces first (distinct concepts, proper nouns) then the rest of the puzzle becomes easier to solve.  Much like doing the edges first then getting down to the hard pieces last who have but one place to go.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Technical Issues\n",
    "\n",
    "languages like Arabic (so I'm told) where words like \"you\" are embedded in the verb itself.  Thus the verb then is not one word but changes based on the pronoun.  \n",
    "\n",
    "And in Chinese it is based on characters (so I understand).\n",
    "\n",
    "And of course there is idioms and words that do not translate to another word.\n",
    "\n",
    "But we do not want word to word translation.  What we want is a concept in one language is \"close\" to the same concept in another language.  This is represented in vector space.  That is a series of numbers like [1,1,2,1,1,1] is similar to [1,1,2.31,1,1,1] but not to [2,2,1,1,1,2]\n",
    "\n",
    "We want to do this automatically and statistically and this is a well researched technique.  In short we feed an entire corpus like the Bible into a neural network and it learns the vector space.  Then we can take a word like \"green\" and find the closest word like \"blue\" or \"yellow\".  The question remains how to get green to mean \"vert\" (thus aligning it across languages).  We have a unique advantage in that the Bible is chunked into the same verses (essentially but statistically consistently enough).  So we can \"align\" all languages to the same in a unique way.\n",
    "\n",
    "WHY?  If all our words (or more accurately concepts) are aligned with each other than when we say \"For God so loved the world\" we get the same meaning in all languages.  This is the first step to AI supported language.  Without this alignment then we are training each language by itself and are unable to leverage the big data findings we start to get from language families sharing a grammer and nuances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Technical Issues: Small common words with little meaning on own.\n",
    "\n",
    "Because we are grabbing the entire verse there is more meaning in that verse than the word we want.  In the case of common words with little distinct meaning on their own like \"the\", \"his\", \"you\" these will get lost.  I have tried to remediate this by giving them many verses to match with.  Presumably the other language will also have the same verses but in reality many languages use different conjunctions/stems with these words and so they may not align on the same verses.  \n",
    "\n",
    "A potential solution is to consider these some of the inner pieces of a jigsaw puzzle.  We could first resolve the easier words (which we could determine statistically by the nearness to other words) then use the approach in the book _create_glossary) but translate all the known words first, thus leaving the small words which will now have a context of how they are used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Potential Issues\n",
    " - Ambiguity: Many words have multiple meanings, and their usage may differ even within the same document based on the context. It might be challenging to align these types of words across languages effectively.\n",
    " - Non-Biblical Texts: The Bible, while extensive, is a specific type of text with its own set of linguistic features and themes. How well the method generalizes to other kinds of texts is an open question.\n",
    " - Linguistic Drift: Languages evolve over time, and what might be a perfect alignment today may not hold in the future.\n",
    " - If there are minimum verses and usally appear in the same phrase then it may be bound closer to that phrase ex. \"you have heard that it was said but I say to you\" may map \"it was said\" to \"I say to you\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Novel way to Align Languages\n",
    "\n",
    "Since all Bible verses are aligned by verses we can use that alignment to extract a meaning.  To do this we will follow these steps\n",
    "\n",
    "1. Count the occurances of all words in a language (I will show later we will actualy count ngrams to capture meanings but more on this in a moment)\n",
    "1. Starting with the most common word search for all verses that contain that word\n",
    "1. Look up all those verses in a common language (not the source language as openAI sentence embeddings are not aligned the same for each verse across languages - see universal_embeddings.ipynb), it is the common language that will give us a shared meaning of a word across all languages\n",
    "1. Generate a sentence embedding that covers all the verses and thus embeds all the meaning of those verses (this will not give us the meaning of the word but the meaning of that group of verses will be shared with all other languages even when partial as in the case of \"the\" in greek having 16 variations, thus it will be similar but not the same which is good as each langauge has slighly different nuances per word)\n",
    "\n",
    "A potential extension of this work\n",
    "1. Take FastText and all Bibles across all languages train new embeddings\n",
    "1. Using the embeddings above as pre-trained embeddings\n",
    "1. Thus deepening the meaning of each word to actually be the words meaning for that language but with a starting point of it's shared meaning as used in verses across all languages.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some defaults\n",
    "TRAINING_SOURCE = ['MAT',\n",
    " 'MRK',\n",
    " 'LUK',\n",
    " 'JHN',\n",
    " 'ACT',\n",
    " 'ROM',\n",
    " '1CO',\n",
    " '2CO',\n",
    " 'GAL',\n",
    " 'EPH',\n",
    " 'PHP',\n",
    " 'COL',\n",
    " '1TH',\n",
    " '2TH',\n",
    " '1TI',\n",
    " '2TI',\n",
    " 'TIT',\n",
    " 'PHM',\n",
    " 'HEB',\n",
    " 'JAS',\n",
    " '1PE',\n",
    " '2PE',\n",
    " '1JN',\n",
    " '2JN',\n",
    " '3JN',\n",
    " 'JUD',\n",
    " 'REV'\n",
    " ]\n",
    "VERSIONS = ['eng-web','eng-asv','eng-kjv2006','engBBE','hin2017', 'arbnav', 'latVUC', 'amo']\n",
    "COMMON_VERSION = 'eng-asv'\n",
    "MAX_VERSES = 50\n",
    "MODEL = 'text-embedding-ada-002'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and fixes to pathing\n",
    "%reload_ext autoreload\n",
    "import sys\n",
    "sys.path.append('../lib')\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken\n",
    "import openai, time, os\n",
    "from openai.error import RateLimitError, OpenAIError\n",
    "from config import get_config\n",
    "from collections import defaultdict\n",
    "from cipher import substitution_cipher\n",
    "\n",
    "# openai.api_type = os.environ[\"OPENAI_API_TYPE\"] = get_config('openai')['api_type']\n",
    "# openai.api_base = os.environ[\"OPENAI_API_BASE\"] = get_config('openai')['api_base']\n",
    "# openai.api_key = os.environ[\"OPENAI_API_KEY\"] = get_config('openai')['api_key']\n",
    "# openai.api_version = os.environ[\"OPENAI_API_VERSION\"] = get_config('openai')['api_version']\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"] = get_config('embeddings')['api_key']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get our Bibles\n",
    "\n",
    "https://github.com/ChrisPriebe/BibleTranslation/blob/master/get_bible.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vref</th>\n",
       "      <th>book</th>\n",
       "      <th>chapter</th>\n",
       "      <th>verse</th>\n",
       "      <th>eng-web</th>\n",
       "      <th>eng-asv</th>\n",
       "      <th>eng-kjv2006</th>\n",
       "      <th>engBBE</th>\n",
       "      <th>hin2017</th>\n",
       "      <th>arbnav</th>\n",
       "      <th>latVUC</th>\n",
       "      <th>amo</th>\n",
       "      <th>source_content</th>\n",
       "      <th>birrig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GEN 1:1</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>In the beginning, God created the heavens and ...</td>\n",
       "      <td>In the beginning God created the heavens and t...</td>\n",
       "      <td>In the beginning God created the heaven and th...</td>\n",
       "      <td>At the first God made the heaven and the earth.</td>\n",
       "      <td>आदि में परमेश्‍वर ने आकाश और पृथ्वी की सृष्टि ...</td>\n",
       "      <td>فِي الْبَدْءِ خَلَقَ اللهُ السَّمَاوَاتِ وَالأ...</td>\n",
       "      <td>In principio creavit Deus cælum et terram.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>בְּרֵאשִׁ֖ית  בָּרָ֣א  אֱלֹהִ֑ים  אֵ֥ת  הַשָּׁ...</td>\n",
       "      <td>El lxi sovzl Guw newi lxi xiemir erw lxi ievlx.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GEN 1:2</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>The earth was formless and empty. Darkness was...</td>\n",
       "      <td>And the earth was waste and void; and darkness...</td>\n",
       "      <td>And the earth was without form, and void; and ...</td>\n",
       "      <td>And the earth was waste and without form; and ...</td>\n",
       "      <td>पृथ्वी बेडौल और सुनसान पड़ी थी, और गहरे जल के ...</td>\n",
       "      <td>وَإِذْ كَانَتِ الأَرْضُ مُشَوَّشَةً وَمُقْفِرَ...</td>\n",
       "      <td>Terra autem erat inanis et vacua, et tenebræ e...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וְהָאָ֗רֶץ  הָיְתָ֥ה  תֹ֨הוּ֙  וָבֹ֔הוּ  וְחֹ֖...</td>\n",
       "      <td>Erw lxi ievlx hez hezli erw holxual suvn; erw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GEN 1:3</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>God said, “Let there be light,” and there was ...</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>And God said, Let there be light: and there wa...</td>\n",
       "      <td>तब परमेश्‍वर ने कहा, “उजियाला हो*,” तो उजियाला...</td>\n",
       "      <td>أَمَرَ اللهُ: «لِيَكُنْ نُورٌ». فَصَارَ نُورٌ،</td>\n",
       "      <td>Dixitque Deus: Fiat lux. Et facta est lux.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וַיֹּ֥אמֶר  אֱלֹהִ֖ים  יְהִ֣י  א֑וֹר  וַֽיְהִי...</td>\n",
       "      <td>Erw Guw zeow, Pil lxivi fi pogxl: erw lxivi he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GEN 1:4</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>God saw the light, and saw that it was good. G...</td>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>And God saw the light, that it was good: and G...</td>\n",
       "      <td>And God, looking on the light, saw that it was...</td>\n",
       "      <td>और परमेश्‍वर ने उजियाले को देखा कि अच्छा है*; ...</td>\n",
       "      <td>وَرَأَى اللهُ النُّورَ فَاسْتَحْسَنَهُ وَفَصَل...</td>\n",
       "      <td>Et vidit Deus lucem quod esset bona: et divisi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וַיַּ֧רְא  אֱלֹהִ֛ים  אֶת־ הָא֖וֹר  כִּי־ ט֑וֹ...</td>\n",
       "      <td>Erw Guw, puucorg ur lxi pogxl, zeh lxel ol hez...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GEN 1:5</td>\n",
       "      <td>GEN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>God called the light “day”, and the darkness h...</td>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>And God called the light Day, and the darkness...</td>\n",
       "      <td>Naming the light, Day, and the dark, Night. An...</td>\n",
       "      <td>और परमेश्‍वर ने उजियाले को दिन और अंधियारे को ...</td>\n",
       "      <td>وَسَمَّى اللهُ النُّورَ نَهَاراً، أَمَّا الظَّ...</td>\n",
       "      <td>Appellavitque lucem Diem, et tenebras Noctem: ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>וַיִּקְרָ֨א  אֱלֹהִ֤ים׀ לָאוֹר֙  י֔וֹם  וְלַחֹ...</td>\n",
       "      <td>Renorg lxi pogxl, Wej, erw lxi wevc, Rogxl. Er...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      vref book  chapter  verse  \\\n",
       "0  GEN 1:1  GEN        1      1   \n",
       "1  GEN 1:2  GEN        1      2   \n",
       "2  GEN 1:3  GEN        1      3   \n",
       "3  GEN 1:4  GEN        1      4   \n",
       "4  GEN 1:5  GEN        1      5   \n",
       "\n",
       "                                             eng-web  \\\n",
       "0  In the beginning, God created the heavens and ...   \n",
       "1  The earth was formless and empty. Darkness was...   \n",
       "2  God said, “Let there be light,” and there was ...   \n",
       "3  God saw the light, and saw that it was good. G...   \n",
       "4  God called the light “day”, and the darkness h...   \n",
       "\n",
       "                                             eng-asv  \\\n",
       "0  In the beginning God created the heavens and t...   \n",
       "1  And the earth was waste and void; and darkness...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                         eng-kjv2006  \\\n",
       "0  In the beginning God created the heaven and th...   \n",
       "1  And the earth was without form, and void; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God saw the light, that it was good: and G...   \n",
       "4  And God called the light Day, and the darkness...   \n",
       "\n",
       "                                              engBBE  \\\n",
       "0    At the first God made the heaven and the earth.   \n",
       "1  And the earth was waste and without form; and ...   \n",
       "2  And God said, Let there be light: and there wa...   \n",
       "3  And God, looking on the light, saw that it was...   \n",
       "4  Naming the light, Day, and the dark, Night. An...   \n",
       "\n",
       "                                             hin2017  \\\n",
       "0  आदि में परमेश्‍वर ने आकाश और पृथ्वी की सृष्टि ...   \n",
       "1  पृथ्वी बेडौल और सुनसान पड़ी थी, और गहरे जल के ...   \n",
       "2  तब परमेश्‍वर ने कहा, “उजियाला हो*,” तो उजियाला...   \n",
       "3  और परमेश्‍वर ने उजियाले को देखा कि अच्छा है*; ...   \n",
       "4  और परमेश्‍वर ने उजियाले को दिन और अंधियारे को ...   \n",
       "\n",
       "                                              arbnav  \\\n",
       "0  فِي الْبَدْءِ خَلَقَ اللهُ السَّمَاوَاتِ وَالأ...   \n",
       "1  وَإِذْ كَانَتِ الأَرْضُ مُشَوَّشَةً وَمُقْفِرَ...   \n",
       "2     أَمَرَ اللهُ: «لِيَكُنْ نُورٌ». فَصَارَ نُورٌ،   \n",
       "3  وَرَأَى اللهُ النُّورَ فَاسْتَحْسَنَهُ وَفَصَل...   \n",
       "4  وَسَمَّى اللهُ النُّورَ نَهَاراً، أَمَّا الظَّ...   \n",
       "\n",
       "                                              latVUC  amo  \\\n",
       "0         In principio creavit Deus cælum et terram.  NaN   \n",
       "1  Terra autem erat inanis et vacua, et tenebræ e...  NaN   \n",
       "2         Dixitque Deus: Fiat lux. Et facta est lux.  NaN   \n",
       "3  Et vidit Deus lucem quod esset bona: et divisi...  NaN   \n",
       "4  Appellavitque lucem Diem, et tenebras Noctem: ...  NaN   \n",
       "\n",
       "                                      source_content  \\\n",
       "0  בְּרֵאשִׁ֖ית  בָּרָ֣א  אֱלֹהִ֑ים  אֵ֥ת  הַשָּׁ...   \n",
       "1  וְהָאָ֗רֶץ  הָיְתָ֥ה  תֹ֨הוּ֙  וָבֹ֔הוּ  וְחֹ֖...   \n",
       "2  וַיֹּ֥אמֶר  אֱלֹהִ֖ים  יְהִ֣י  א֑וֹר  וַֽיְהִי...   \n",
       "3  וַיַּ֧רְא  אֱלֹהִ֛ים  אֶת־ הָא֖וֹר  כִּי־ ט֑וֹ...   \n",
       "4  וַיִּקְרָ֨א  אֱלֹהִ֤ים׀ לָאוֹר֙  י֔וֹם  וְלַחֹ...   \n",
       "\n",
       "                                              birrig  \n",
       "0    El lxi sovzl Guw newi lxi xiemir erw lxi ievlx.  \n",
       "1  Erw lxi ievlx hez hezli erw holxual suvn; erw ...  \n",
       "2  Erw Guw zeow, Pil lxivi fi pogxl: erw lxivi he...  \n",
       "3  Erw Guw, puucorg ur lxi pogxl, zeh lxel ol hez...  \n",
       "4  Renorg lxi pogxl, Wej, erw lxi wevc, Rogxl. Er...  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: remplace this with download from ecorpus and prepare it\n",
    "\n",
    "# Read the data/berrig.csv file into dataframe\n",
    "df = pd.read_csv('../data/birrig.csv')\n",
    "df.rename(columns={df.columns[0]: 'vref'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN SIZE: 7957 verses\n"
     ]
    }
   ],
   "source": [
    "training_df = df[df['book'].isin(TRAINING_SOURCE)]\n",
    "print(f\"TRAIN SIZE: {len(training_df)} verses\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "        # Remove punc and lowercase all words\n",
    "        # TODO: for more languages you can use unicode base tools to look at the type of char it is\n",
    "        # and if type of punc then skip it.\n",
    "        return \" \".join([ word.lower().strip('.,;!?[]{}()\\'\"\\\\') for word in text.split()])\n",
    "\n",
    "\n",
    "def get_ngrams(df, column_name, max_ngarms_length=10, trim_bigrams=5, normalize=False):\n",
    "    ngrams = defaultdict(int)\n",
    "    for index, row in df.iterrows():\n",
    "        text = normalize_text and normalize_text(row[column_name]) or row[column_name]\n",
    "        words = text.split()\n",
    "        for i, word in enumerate(words):\n",
    "            for j in range(0, max_ngarms_length):\n",
    "                if i+j < len(words):\n",
    "                    ngrams[\" \".join(words[i:i+j+1])] += 1\n",
    "\n",
    "    # Remove any ngrams that have only one count\n",
    "    if trim_bigrams:\n",
    "        ngrams = {ngram: count for ngram, count in ngrams.items() if count >= trim_bigrams or \" \" not in ngram}\n",
    "\n",
    "    # Sort the ngrams by frequency\n",
    "    sorted_ngrams = sorted(ngrams.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_ngrams\n",
    "\n",
    "def get_embedding(inputs, words_df):\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            result = openai.Embedding.create(input=[input for (_, input) in inputs], model=MODEL)['data']\n",
    "            for i, (word, _) in enumerate(inputs):\n",
    "                embeddings.loc[len(embeddings)] = [word['word'], result[i]['embedding']]\n",
    "            return\n",
    "        \n",
    "        except RateLimitError as e:\n",
    "            print(f\"Rate Limit Error: {e}\")\n",
    "            time.sleep(30)\n",
    "            continue\n",
    "\n",
    "        except Exception as e:\n",
    "            if \"502 Bad Gateway\" in e.message:\n",
    "                print(\"Bad Gateway Error\")\n",
    "                time.sleep(30)\n",
    "                continue\n",
    "\n",
    "            print(\"Error\", e)\n",
    "            return\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "\n",
    "for VERSION in VERSIONS:\n",
    "    print(\"Processing\", VERSION)\n",
    "    # remove nan from training_df[VERSION]\n",
    "    df = training_df[training_df[VERSION].notna()]\n",
    "    df = df[df[COMMON_VERSION].notna()]\n",
    "    df.loc[:, 'normalized'] = df[VERSION].apply(normalize_text)\n",
    "\n",
    "    words = get_ngrams(df, 'normalized')\n",
    "    # convert words to dataframe\n",
    "    words_df = pd.DataFrame(words, columns=['word', 'frequency'])\n",
    "    words_df['grams'] = words_df['word'].apply(lambda x: len(x.split()))\n",
    "    # limit to words that appear at least 3 times\n",
    "    words_df = words_df[words_df['frequency'] > 2]\n",
    "\n",
    "    print(\"Total Words\", len(words_df))\n",
    "\n",
    "    current_tokens = 0\n",
    "    inputs = []\n",
    "    embeddings = pd.DataFrame(columns=['word', 'embeddings'])\n",
    "    total_tokens = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Create our sentence embedding\n",
    "    for _, word in words_df.iterrows():\n",
    "        # Find all the occurrences of the word in the training data\n",
    "        verses = df[df['normalized'].str.contains(word['word'])][['vref', COMMON_VERSION]]\n",
    "\n",
    "        # Create the input for the embedding\n",
    "        keep = len(verses) > MAX_VERSES and MAX_VERSES / len(verses) or 1.0\n",
    "        while input is None or len(tokenizer.encode(input)) > 7000:\n",
    "            # Cut the input by 20% until it's under 8000 tokens\n",
    "            #input = '\\n'.join([f\"{verse['vref']}\\t{verse[COMMON_VERSION]}\" for _, verse in verses.sample(frac=keep, random_state=1).iterrows()])\n",
    "            input = '\\n'.join([verse[COMMON_VERSION] for _, verse in verses.sample(frac=keep, random_state=1).iterrows()])\n",
    "            keep = keep * 0.8\n",
    "\n",
    "        current_tokens += len(tokenizer.encode(input))\n",
    "        if current_tokens > 7000:\n",
    "            total % 100 == 0 and print(f\"Getting Embeddings\\t {total} of {len(words_df)} {total/len(words_df)}%\")\n",
    "            get_embedding(inputs, words_df)\n",
    "            total_tokens += current_tokens\n",
    "            current_tokens = len(tokenizer.encode(input))\n",
    "            inputs = []\n",
    "\n",
    "    \n",
    "        inputs.append((word, input))\n",
    "        total += 1\n",
    "        input = None\n",
    "\n",
    "    # One last call to finish it off\n",
    "    get_embedding(inputs, words_df)\n",
    "    total_tokens += current_tokens\n",
    "\n",
    "    print(f\"DONE {VERSION}\\ttotal tokens {total_tokens}\\tAprox Cost ${total_tokens*0.0000001}\")\n",
    "\n",
    "    words_df['version'] = VERSION\n",
    "    # join the embeddings to the words_df on word\n",
    "    df = words_df.merge(embeddings, on='word', how='left')\n",
    "    df.to_json(f'../data/{VERSION}_embeddings.jsonl', orient='records', lines=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>frequency</th>\n",
       "      <th>grams</th>\n",
       "      <th>embedding</th>\n",
       "      <th>version</th>\n",
       "      <th>embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>853</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eng-web</td>\n",
       "      <td>[0.006576310843229294, 0.0028880443423986435, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>637</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eng-web</td>\n",
       "      <td>[0.014513843692839146, -0.008422507904469967, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to</td>\n",
       "      <td>477</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eng-web</td>\n",
       "      <td>[0.011934485286474228, -0.0156710147857666, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>he</td>\n",
       "      <td>404</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eng-web</td>\n",
       "      <td>[0.013367237523198128, -0.01836239919066429, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>him</td>\n",
       "      <td>357</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eng-web</td>\n",
       "      <td>[0.006650935858488083, -0.005694134626537561, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>eleven</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engBBE</td>\n",
       "      <td>[0.00019006912771146744, -0.02636721171438694,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>faith:</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engBBE</td>\n",
       "      <td>[-0.01304407138377428, -0.008051815442740917, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>languages</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engBBE</td>\n",
       "      <td>[-0.01304407138377428, -0.008051815442740917, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3334</th>\n",
       "      <td>snakes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engBBE</td>\n",
       "      <td>[-0.003936239052563906, 0.0005580236902460456,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>poison</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>engBBE</td>\n",
       "      <td>[-0.003936239052563906, 0.0005580236902460456,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10126 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           word  frequency  grams  embedding  version  \\\n",
       "0           the        853      1        NaN  eng-web   \n",
       "1           and        637      1        NaN  eng-web   \n",
       "2            to        477      1        NaN  eng-web   \n",
       "3            he        404      1        NaN  eng-web   \n",
       "4           him        357      1        NaN  eng-web   \n",
       "...         ...        ...    ...        ...      ...   \n",
       "3331     eleven          1      1        NaN   engBBE   \n",
       "3332     faith:          1      1        NaN   engBBE   \n",
       "3333  languages          1      1        NaN   engBBE   \n",
       "3334     snakes          1      1        NaN   engBBE   \n",
       "3335     poison          1      1        NaN   engBBE   \n",
       "\n",
       "                                             embeddings  \n",
       "0     [0.006576310843229294, 0.0028880443423986435, ...  \n",
       "1     [0.014513843692839146, -0.008422507904469967, ...  \n",
       "2     [0.011934485286474228, -0.0156710147857666, -0...  \n",
       "3     [0.013367237523198128, -0.01836239919066429, -...  \n",
       "4     [0.006650935858488083, -0.005694134626537561, ...  \n",
       "...                                                 ...  \n",
       "3331  [0.00019006912771146744, -0.02636721171438694,...  \n",
       "3332  [-0.01304407138377428, -0.008051815442740917, ...  \n",
       "3333  [-0.01304407138377428, -0.008051815442740917, ...  \n",
       "3334  [-0.003936239052563906, 0.0005580236902460456,...  \n",
       "3335  [-0.003936239052563906, 0.0005580236902460456,...  \n",
       "\n",
       "[10126 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO \n",
    "# compare embeddings to see which are matched\n",
    "# test with other languages (like the greek the issue where has many forms)\n",
    "\n",
    "# load the embeddings and concat them\n",
    "df = pd.concat([\n",
    "    pd.read_json('../data/eng-web_embeddings.jsonl', lines=True),\\\n",
    "    pd.read_json('../data/birrig_embeddings.jsonl', lines=True),\\\n",
    "    pd.read_json('../data/engBBE_embeddings.jsonl', lines=True) \\\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop column embedding\n",
    "df.drop(columns=['embedding'], inplace=True)\n",
    "# drop rows with no value in embeddings\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because we loaded it from csv we need to format it back to a list\n",
    "df['embeddings'] = df['embeddings'].apply(lambda x: [float(i) for i in x.strip('[]').split(',')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 'why' from eng-web which is most similar to\n",
      "                  word  version  grams\n",
      "1000               why  eng-web      1\n",
      "205                why   engBBE      1\n",
      "205                hxj   birrig      1\n",
      "489            “why do  eng-web      2\n",
      "202               “why  eng-web      1\n",
      "706        “why do you  eng-web      3\n",
      "548            why are   engBBE      2\n",
      "548            hxj evi   birrig      2\n",
      "2147            reason  eng-web      1\n",
      "1326  them why are you   engBBE      4\n",
      "Processing 'to those' from eng-web which is most similar to\n",
      "                         word  version  grams\n",
      "1001                 to those  eng-web      2\n",
      "1002             to those who  eng-web      3\n",
      "996         who were with him   engBBE      4\n",
      "993       lxuzi hxu hivi holx   birrig      4\n",
      "994   lxuzi hxu hivi holx xon   birrig      5\n",
      "995             hxu hivi holx   birrig      3\n",
      "996         hxu hivi holx xon   birrig      4\n",
      "995             who were with   engBBE      3\n",
      "993       those who were with   engBBE      4\n",
      "994   those who were with him   engBBE      5\n",
      "Processing 'to those who' from eng-web which is most similar to\n",
      "                         word  version  grams\n",
      "1001                 to those  eng-web      2\n",
      "1002             to those who  eng-web      3\n",
      "996         who were with him   engBBE      4\n",
      "993       lxuzi hxu hivi holx   birrig      4\n",
      "994   lxuzi hxu hivi holx xon   birrig      5\n",
      "995             hxu hivi holx   birrig      3\n",
      "996         hxu hivi holx xon   birrig      4\n",
      "995             who were with   engBBE      3\n",
      "993       those who were with   engBBE      4\n",
      "994   those who were with him   engBBE      5\n",
      "Processing 'son of man is' from eng-web which is most similar to\n",
      "                   word  version  grams\n",
      "1376          of man is   engBBE      3\n",
      "1377             ner oz   birrig      2\n",
      "1003      son of man is  eng-web      4\n",
      "1374  the son of man is   engBBE      5\n",
      "1375      son of man is   engBBE      4\n",
      "1377             man is   engBBE      2\n",
      "1374  lxi zur us ner oz   birrig      5\n",
      "1375      zur us ner oz   birrig      4\n",
      "1376          us ner oz   birrig      3\n",
      "1004          of man is  eng-web      3\n",
      "Processing 'of man is' from eng-web which is most similar to\n",
      "                   word  version  grams\n",
      "1004          of man is  eng-web      3\n",
      "1005             man is  eng-web      2\n",
      "1374  lxi zur us ner oz   birrig      5\n",
      "1377             man is   engBBE      2\n",
      "1376          of man is   engBBE      3\n",
      "1377             ner oz   birrig      2\n",
      "1375      son of man is   engBBE      4\n",
      "1374  the son of man is   engBBE      5\n",
      "1003      son of man is  eng-web      4\n",
      "1375      zur us ner oz   birrig      4\n",
      "Processing 'man is' from eng-web which is most similar to\n",
      "                   word  version  grams\n",
      "1005             man is  eng-web      2\n",
      "1004          of man is  eng-web      3\n",
      "1375      son of man is   engBBE      4\n",
      "1003      son of man is  eng-web      4\n",
      "1377             ner oz   birrig      2\n",
      "1375      zur us ner oz   birrig      4\n",
      "1376          of man is   engBBE      3\n",
      "1377             man is   engBBE      2\n",
      "1374  the son of man is   engBBE      5\n",
      "1374  lxi zur us ner oz   birrig      5\n",
      "Processing 'there was a' from eng-web which is most similar to\n",
      "                   word  version  grams\n",
      "1006        there was a  eng-web      3\n",
      "1378        there was a   engBBE      3\n",
      "1378        lxivi hez e   birrig      3\n",
      "2088        hez e gviel   birrig      3\n",
      "2087  lxivi hez e gviel   birrig      4\n",
      "2088        was a great   engBBE      3\n",
      "2087  there was a great   engBBE      4\n",
      "2202             ceased  eng-web      1\n",
      "674           there was   engBBE      2\n",
      "674           lxivi hez   birrig      2\n",
      "Processing 'looked around' from eng-web which is most similar to\n",
      "                     word  version  grams\n",
      "1007        looked around  eng-web      2\n",
      "850         puucorg vuarw   birrig      2\n",
      "850         looking round   engBBE      2\n",
      "2431  looking round about   engBBE      3\n",
      "2431  puucorg vuarw efual   birrig      3\n",
      "499                looked  eng-web      1\n",
      "143                around  eng-web      1\n",
      "3277                 puuc   birrig      1\n",
      "3277                 look   engBBE      1\n",
      "236               looking   engBBE      1\n",
      "Processing 'of their' from eng-web which is most similar to\n",
      "                 word  version  grams\n",
      "1008         of their  eng-web      2\n",
      "1213             seki   birrig      1\n",
      "1213             face   engBBE      1\n",
      "1628     took him and   engBBE      3\n",
      "1628     luuc xon erw   birrig      3\n",
      "1352  and said to him   engBBE      4\n",
      "1352  erw zeow lu xon   birrig      4\n",
      "3307               xe   birrig      1\n",
      "3307               ha   engBBE      1\n",
      "3275            come:   engBBE      1\n",
      "Processing 'it out' from eng-web which is most similar to\n",
      "          word  version  grams\n",
      "1009    it out  eng-web      2\n",
      "1389    ol ual   birrig      2\n",
      "1389    it out   engBBE      2\n",
      "2121   to send   engBBE      2\n",
      "2121   lu zirw   birrig      2\n",
      "1758     throw  eng-web      1\n",
      "73    orlu lxi   birrig      2\n",
      "73    into the   engBBE      2\n",
      "163       cast  eng-web      1\n",
      "26         put   engBBE      1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from openai.embeddings_utils import cosine_similarity\n",
    "# convert the DataFrame to a matrix\n",
    "\n",
    "for _, word in df[1000:1010].iterrows():\n",
    "    print(f\"Processing '{word['word']}' from {word['version']} which is most similar to\")\n",
    "    # Remove this verse so we don't get ourselves\n",
    "    df['similarities'] = df.embeddings.apply(lambda x: cosine_similarity(x, word['embeddings']))\n",
    "    print(df.sort_values('similarities', ascending=False)[['word','version', 'grams']].head(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
